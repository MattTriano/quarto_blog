{
 "cells": [
  {
   "cell_type": "raw",
   "id": "103816b3-4b1c-4b9a-9672-f5ee597d5bee",
   "metadata": {},
   "source": [
    "---\n",
    "title: \"Midjourney Experiments\"\n",
    "description: \"How to get started playing with Midjourney\"\n",
    "author: \"Matt Triano\"\n",
    "date: \"07/01/2023\"\n",
    "date-modified: \"07/07/2023\"\n",
    "draft: false\n",
    "image: \"Midjourney_logo_reimagined.png\"\n",
    "categories: [generative AI, midjourney, prompt eng]\n",
    "format:\n",
    "  html:\n",
    "    toc: true\n",
    "    code-fold: true\n",
    "    code-tools: true\n",
    "jupyter:\n",
    "  kernelspec:\n",
    "    name: \"quarto_env\"\n",
    "    language: \"python\"\n",
    "    display_name: \"quarto_env\"\n",
    "execute:\n",
    "  freeze: true\n",
    "  cache: true\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "428a0df3-4f10-425f-b1a7-97ed8ff79e08",
   "metadata": {},
   "source": [
    "Over the past decade, the field of generative AI has made amazing progress. Driven by breakthrough advances in machine learning modeling strategies (primarily the GAN [^1] in 2014 and the Transformer [^2] in 2017) coupled with exponential growth of the amount of available computing power [^3], generative AI applications for different data formats started appearing in the late 2010s and early 2020s. Notable examples include\n",
    "\n",
    "* improvements to Google Translate (2017 [^4]),\n",
    "* fake face generation at [thispersondoesnotexist.com](https://www.thispersondoesnotexist.com) (2019 [^5]),\n",
    "* molecule structure discovery with AlphaFold (2020 [^6]),\n",
    "* text-to-speech in different voices with tools like [Uberduck.ai](https://uberduck.ai) (2020/2021 [^7]),\n",
    "* general AI apps in HuggingFace's [Spaces](https://huggingface.co/spaces) sandbox (2021 [^8]), and\n",
    "* text-to-image services (major options: DALL-E, Stable Diffusion, and Midjourney, 2021).\n",
    "\n",
    "Over the past two years, text-to-image platforms have jockeyed to lead that market with DALL-E generating a massive amount of initial hype, but Midjourney has emerged as the clearly superior service, despite having the least conventional workflow.\n",
    "\n",
    "![midjourney search frequency](GoogleTrends_generative_ai_image_services.png){width=80%}\n",
    "\n",
    "Midjourney's model was the first to reliably produce human forms (including well-formed hands, a difficulty for prior models), its best outputs are unambiguously superior to the best outputs from any other model, and it's tremendously fun to play with. Midjourney isn't free to use (the [least expensive tier](https://docs.midjourney.com/docs/plans) is \\\\$10/mo), but it's free to look at (or search) the endless stream of images generated by other Midjourney users, it's easy to unsubscribe, and it's hard to not get at least \\\\$10 worth of fun out of the service.\n",
    "\n",
    "In this post, I'll point out the important parts of the interface, walk through setup and (un)subscription, demonstrate useful commands, and show examples of what Midjourney can do."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdeab0fd-9e21-4955-8ace-959be34b852a",
   "metadata": {},
   "source": [
    "## Midjourney Interface\n",
    "\n",
    "The Discord interface is packed with buttons, lists, and inputs, but you only need to know 5 areas you need to know.\n",
    "\n",
    "1. **The Midjourney server icon**\n",
    "    * This, the ship icon, should always be selected.\n",
    "2. **The Channel list**\n",
    "    * Pick any \"newbies-##\" or \"general-##\" channel.\n",
    "3. **The Prompt input**\n",
    "    * This is where you enter prompts or commands for the **Midjourney Bot** to handle.\n",
    "4. **The Search bar**\n",
    "    * You can search through all public images in the Midjourney server.\n",
    "5. **The message display space**\n",
    "    * This is where you'll find all (public) images generated in the selected channel (either by you or other users).\n",
    "\n",
    "![Interface](Midjourney_interface_annotated.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "404e0e34-616d-4a73-a7fb-673570b6ac3a",
   "metadata": {},
   "source": [
    "# Setup and Subscription\n",
    "\n",
    "Midjourney runs out of the Midjourney Discord server, so you will need to make a Discord account and then join [Midjourney's Server](https://discord.com/servers/midjourney-662267976984297473). Log in to [Discord](https://discord.com/) (you can use Discord in your browser), go to the Midjourney Server, go to any newbie channel, and enter the **/subscribe** command. This will produce a link (specific to your Discord account) for the subscription page. Go to that link and pick a plan.\n",
    "\n",
    "The [least expensive tier](https://docs.midjourney.com/docs/plans) is $10 per month (for about 250 images per month) and you can have full usage rights*[^9] of the images you generate (at least as of 2023; the legal status of this technology may change in a few years[^10])."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7dabced-1151-4bd2-958b-bc30974174c1",
   "metadata": {},
   "source": [
    "## How to unsubscribe\n",
    "\n",
    "To unsubscribe, go to the [Midjourney account page](https://www.midjourney.com/account/), click **Cancel Plan** (1, then 2), and then confirm cancellation in the popup. If you've used less than 20 GPU minutes (~30 images) in the billing period, you can choose to get a refund."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "294246a1-c290-4a35-aaa9-a3a2eb396dee",
   "metadata": {},
   "source": [
    "![](Midjourney_subscription_page_annotated.png){width=80% fig-alt=\"Account page\"}\n",
    "\n",
    "![](Midjourney_cancellation_confirmation.png){fig-alt=\"Cancellation Confirmation\"}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea460595-e51e-48ba-824d-8d7535254785",
   "metadata": {},
   "source": [
    "# Commands\n",
    "\n",
    "* **/imagine**: This command lets you enter an image-generating prompt. It's the main command you'll use.\n",
    "* **/info**: This command shows the number of GPU minutes you've used that month as well as basic subscription information for your account.\n",
    "* **/subscribe**: Enter this command to get a link to the subscription page for your Discord account. Use this to subscribe to a plan.\n",
    "* **/help**: This command shows basic commands and tips.\n",
    "\n",
    "There are a [few more commands](https://docs.midjourney.com/docs/command-list), but you'll mainly use **/imagine** and **/info**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa7eb8f3-da84-44f5-9453-3c1f7bcb22d6",
   "metadata": {},
   "source": [
    "# Prompt Parameters\n",
    "\n",
    "You can add parameters on to the end of your image prompt to \n",
    "\n",
    "## Aspect Ratio\n",
    "The desired aspect ratio for an image.\n",
    "\n",
    "* Parameter: `--ar l:w` or `--aspect w:h` (where `w` is width `h` is height)\n",
    "* Examples:\n",
    "    * `--ar 1:1` (square, default)\n",
    "    * `--ar 9:16` (good for stories; typical phone screen aspect ratio)\n",
    "    * `--ar 19:10` (good for covers or landscapes)\n",
    "    * `--ar 4:5` (good for portraits)\n",
    "\n",
    "## Quality\n",
    "Controls the rendering quality (i.e., amount of CPU time spent generating an image).\n",
    "\n",
    "* Parameter: `--q value` or `--quality value`\n",
    "* Accepted `value`s: [0.25, 0.5, 1] (higher values = higher quality, default is 1)\n",
    "* Example: `--q 0.25`\n",
    "\n",
    "## Stylize\n",
    "Influences how strongly Midjourney stylizes an image.\n",
    "\n",
    "* Parameter: `--s value` or `--stylize value`\n",
    "* Accepted `value`s: 0 to 1000 (lower values = simpler, higher values = more intricate, default is 100)\n",
    "* Example: `--s 1000`\n",
    "\n",
    "## Negative prompting\n",
    "Allows you to strongly signal something should be excluded from an image.\n",
    "\n",
    "* Parameter: `--no value`\n",
    "* Accepted `value`s: Any text.\n",
    "* Example: `--no plants`\n",
    "\n",
    "The full list of parameters is available [here](https://docs.midjourney.com/docs/parameter-list)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "523cba41-1507-4376-a23a-3e4a02e71c37",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "# Neat Prompt Tags\n",
    "\n",
    "## Papercut\n",
    "\n",
    "![](papercut_prompt.png)\n",
    "\n",
    "## In the style of ...\n",
    "\n",
    "![](imgs/in_the_style_of_van_gogh_art_deco.png)\n",
    "\n",
    "I'll keep adding to the **Neat Prompt Tags** section."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01d50b70-9b47-4474-bc59-2df2a0d49a1b",
   "metadata": {},
   "source": [
    "# Summary\n",
    "\n",
    "It's a neat tool, and it makes it very easy to generate custom graphics (if you don't need it to include exact words or accurate diagrams). I'm not sure how it will imapact the market prospects for graphic artists, or whether it will be legal in a few years (pending lawsuits or legislation), but it's a lot of fun today."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8500cff-3dd7-4073-94b2-9634977a9361",
   "metadata": {},
   "source": [
    "[^1]: Ian Goodfellow et. al's paper (2014) presenting the [Generative Adversarial Network](https://arxiv.org/pdf/1406.2661.pdf) (or GAN) deep learning model architecture.\n",
    "\n",
    "[^2]: Google's famous [Attention is all you need](https://arxiv.org/pdf/1706.03762.pdf) paper (2017) which presented the Transformer deep learning model architecture.\n",
    "\n",
    "[^3]: OpenAI's [2018 analysis](https://openai.com/research/ai-and-compute) of available computing power (or \"compute\") and the amount of compute needed to train models.\n",
    "\n",
    "[^4]: Google's \"Transformer: A Novel Neural Network Architecture for Language Understanding\" [blog post](https://ai.googleblog.com/2017/08/transformer-novel-neural-network.html) (2017) outlining how Google used Transformers to generate more accurate translations in Google Translate.\n",
    "\n",
    "[^5]: Before or on [Feb 11, 2019](https://web.archive.org/web/20190211190227/https://thispersondoesnotexist.com/), [This Person Does Not Exist](https://www.thispersondoesnotexist.com/) came online, generating a hyperrealistic but fake image of a human face.\n",
    "\n",
    "[^6]: In late 2020, DeepMind (a subsidiary of Alphabet) [announced AlphaFold2](https://web.archive.org/web/20201210212032/https://predictioncenter.org/casp14/doc/presentations/2020_12_01_TS_predictor_AlphaFold2.pdf) a reengineered version of their protein-structure prediction system, which uses transformers to predict [much more accurate](https://www.nature.com/articles/s41586-021-03819-2) representations of proteins. Understanding the molecular structure of complex proteins makes it possible to determine what molecular groups are exposed on the outer surface of the molecule, which is useful as only those exposed groups can interact with groups on other molecules.\n",
    "\n",
    "[^7]: In late 2020, [uberduck.ai](https://web.archive.org/web/20201122014832/https://uberduck.ai/) introduced a text-to-speech generation service and (per Uberduck copy) over 5000 voice models that can be used to generate speech from text. In early to mid 2021, this platform gained some press when users started generating not-quite-passable tracks with different rapper voice models.\n",
    "\n",
    "[^8]: Hugging created [spaces](https://huggingface.co/spaces) where users could host small AI applications using existing transformer models on their platform.\n",
    "\n",
    "[^9]: assuming the use is not for commercial purposes by a company earning over \\$1M in gross refenue per year, which must purchase the Pro (\\\\$60/mo) or Mega (\\\\$120/mo) plan. [Source](https://docs.midjourney.com/docs/plans)\n",
    "\n",
    "[^10]: A [lawsuit](https://web.archive.org/web/20230629095344/https://www.theverge.com/2023/1/16/23557098/generative-ai-art-copyright-legal-lawsuit-stable-diffusion-midjourney-deviantart)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11787d57-1e12-4aea-9fb8-7c2b145ba7cc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "(quarto_env)",
   "language": "python",
   "name": "quarto_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
