{
 "cells": [
  {
   "cell_type": "raw",
   "id": "103816b3-4b1c-4b9a-9672-f5ee597d5bee",
   "metadata": {},
   "source": [
    "---\n",
    "title: \"Midjourney Experiments\"\n",
    "description: \"A full tutorial of a basic workflow\"\n",
    "author: \"Matt Triano\"\n",
    "date: \"07/01/2023\"\n",
    "date-modified: \"07/07/2023\"\n",
    "draft: false\n",
    "image: \"pip_setting_(up)_Great_Expectations.png\"\n",
    "categories: [generative AI, midjourney, prompt eng]\n",
    "format:\n",
    "  html:\n",
    "    toc: true\n",
    "    code-fold: true\n",
    "    code-tools: true\n",
    "jupyter:\n",
    "  kernelspec:\n",
    "    name: \"quarto_env\"\n",
    "    language: \"python\"\n",
    "    display_name: \"quarto_env\"\n",
    "execute:\n",
    "  freeze: true\n",
    "  cache: true\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "428a0df3-4f10-425f-b1a7-97ed8ff79e08",
   "metadata": {},
   "source": [
    "Over the past decade, the field of generative AI has made amazing progress. Driven by breakthrough advances in machine learning modeling strategies (primarily the GAN [^0] in 2014 and the Transformer [^1] in 2017) coupled with exponential growth of the amount of available computing power [^2], generative AI applications for different data formats started appearing in the late 2010s and early 2020s. Notable examples include improvements to Google Translate (2017, text/natural language data [^3]), fake face generation (2019, image data [^4]), molecule structure discovery (2020, molecule structure data [^5]), speech generation (2020/2021, audio/speech [^6]), and general AI apps in HuggingFace's **Spaces** sandbox [^7].\n",
    "\n",
    "Over the past year, generative AI has transitioned from a \n",
    "\n",
    "\n",
    "\n",
    "![midjourney search frequency](GoogleTrends_Midjourney.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a51adb87-5430-42e1-9adf-2b412f8429ae",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07a52981-8a0f-4700-af82-f450e2cb12c0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0c61c68-4668-4d76-9477-25cbf2830693",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4745204-4d6c-4a66-b1e5-310f1d1d409f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fae096d9-edd1-48ab-82fe-9ceb73856c97",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a8500cff-3dd7-4073-94b2-9634977a9361",
   "metadata": {},
   "source": [
    "[^0]: Ian Goodfellow et. al's paper (2014) presenting the [Generative Adversarial Network](https://arxiv.org/pdf/1406.2661.pdf) (or GAN) deep learning model architecture.\n",
    "\n",
    "[^1]: Google's famous [Attention is all you need](https://arxiv.org/pdf/1706.03762.pdf) paper (2017) which presented the Transformer deep learning model architecture.\n",
    "\n",
    "[^2]: OpenAI's [2018 analysis](https://openai.com/research/ai-and-compute) of available computing power (or \"compute\") and the amount of compute needed to train models.\n",
    "\n",
    "[^3]: Google's \"Transformer: A Novel Neural Network Architecture for Language Understanding\" [blog post](https://ai.googleblog.com/2017/08/transformer-novel-neural-network.html) (2017) outlining how Google used Transformers to generate more accurate translations in Google Translate.\n",
    "\n",
    "[^4]: Before or on [Feb 11, 2019](https://web.archive.org/web/20190211190227/https://thispersondoesnotexist.com/), [This Person Does Not Exist](https://www.thispersondoesnotexist.com/) came online, generating a hyperrealistic but fake image of a human face.\n",
    "\n",
    "[^5]: In late 2020, DeepMind (a subsidiary of Alphabet) [announced AlphaFold2](https://web.archive.org/web/20201210212032/https://predictioncenter.org/casp14/doc/presentations/2020_12_01_TS_predictor_AlphaFold2.pdf) a reengineered version of their protein-structure prediction system, which uses transformers to predict [much more accurate](https://www.nature.com/articles/s41586-021-03819-2) representations of proteins. Understanding the molecular structure of complex proteins makes it possible to determine what molecular groups are exposed on the outer surface of the molecule, which is useful as only those exposed groups can interact with groups on other molecules.\n",
    "\n",
    "[^6]: In late 2020, [uberduck.ai](https://web.archive.org/web/20201122014832/https://uberduck.ai/) introduced a text-to-speech generation service and (per Uberduck copy) over 5000 voice models that can be used to generate speech from text. In early to mid 2021, this platform gained some press when users started generating not-quite-passable tracks with different rapper voice models.\n",
    "\n",
    "[^7]: Hugging created [spaces](https://huggingface.co/spaces) where users could host small AI applications using existing transformer models on their platform."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "(quarto_env)",
   "language": "python",
   "name": "quarto_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
