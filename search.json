[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "quarto_blog",
    "section": "",
    "text": "Data Quality Monitoring with Great Expectations\n\n\n\n\n\nPart 1: Setup and Setting Expectations\n\n\n\n\n\n\nJun 23, 2023\n\n\nMatt Triano\n\n\n\n\n\n\n  \n\n\n\n\nCensus TIGER Dataset Collector Dev\n\n\n\n\n\nDocumenting the synthesis stage of development\n\n\n\n\n\n\nJun 14, 2023\n\n\nMatt Triano\n\n\n\n\n\n\n  \n\n\n\n\nWorking with Notebooks\n\n\n\n\n\nHow to develop a post in a notebook\n\n\n\n\n\n\nJun 12, 2023\n\n\nMatt Triano\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "Matt Triano is a Data Scientist at the University of Chicago’s Urban Labs. When not innovating on data platforms, Matt enjoys exploring new tech, open source development, and hardware hacking.\n\n\nDePaul University | Chicago, IL M.S. in Computer Science | Sept 2013 - Mar 2018\nPurdue University | West Lafayette, IN B.S. in Physics | Aug 2006 - Dec 2010"
  },
  {
    "objectID": "about.html#education",
    "href": "about.html#education",
    "title": "About",
    "section": "",
    "text": "DePaul University | Chicago, IL M.S. in Computer Science | Sept 2013 - Mar 2018\nPurdue University | West Lafayette, IN B.S. in Physics | Aug 2006 - Dec 2010"
  },
  {
    "objectID": "posts/006_great_expectations_setup/data_monitoring_w_gx_1_setup.html",
    "href": "posts/006_great_expectations_setup/data_monitoring_w_gx_1_setup.html",
    "title": "Data Quality Monitoring with Great Expectations",
    "section": "",
    "text": "Code\nimport datetime as dt\nfrom pathlib import Path\nfrom urllib.request import urlretrieve\n\nimport geopandas as gpd\nimport pandas as pd\nGreat Expectations is an open-source Python-based library that brings the idea of “testing” to your data. It enables you to define expectations for properties of your datasets (like records per batch, distribution of values in a column, columns in a table, etc) and check that the data meets those expectations when the data is updated."
  },
  {
    "objectID": "posts/006_great_expectations_setup/data_monitoring_w_gx_1_setup.html#sample-data-collection-and-preparation",
    "href": "posts/006_great_expectations_setup/data_monitoring_w_gx_1_setup.html#sample-data-collection-and-preparation",
    "title": "Data Quality Monitoring with Great Expectations",
    "section": "Sample Data Collection and Preparation",
    "text": "Sample Data Collection and Preparation\n\n\nCode\nPOST_DIR = Path(\".\").resolve()\nPOST_DATA_DIR = POST_DIR.joinpath(\"data\")\nPOST_DATA_DIR.mkdir(exist_ok=True)\n\n\n\n\nCode\nurl = \"https://data.cityofchicago.org/api/geospatial/4ijn-s7e5?method=export&format=GeoJSON\"\nfull_file_path = POST_DATA_DIR.joinpath(\"full_food_inspections.geojson\")\nif not full_file_path.is_file():\n    urlretrieve(url=url, filename=full_file_path)\nfull_food_inspection_gdf = gpd.read_file(full_file_path)\n\n\n\n\nCode\nprint(full_food_inspection_gdf.shape)\nfull_food_inspection_gdf.head(2)\n\n\n(255573, 21)\n\n\n\n\n\n\n\n\n\nlocation_state\nfacility_type\ncity\nlocation_zip\ninspection_id\nlicense_\nlatitude\nzip\nstate\nlocation_address\n...\naka_name\nrisk\nlongitude\ndba_name\ninspection_date\nresults\ninspection_type\naddress\nviolations\ngeometry\n\n\n\n\n0\nNaN\nRestaurant\nCHICAGO\nNaN\n2577546\n2808408\n41.79300598548857\n60638\nIL\nNaN\n...\nCIAO RAGAZZI\nRisk 1 (High)\n-87.78197456162096\nCIAO RAGAZZI\n2023-06-20\nPass w/ Conditions\nCanvass\n5440 S NARRAGANSETT AVE\n10. ADEQUATE HANDWASHING SINKS PROPERLY SUPPLI...\nPOINT (-87.78197 41.79301)\n\n\n1\nNaN\nRestaurant\nCHICAGO\nNaN\n2577553\n2078887\n41.83100808816173\n60616\nIL\nNaN\n...\nCHI SOX BAR & GRILL\nRisk 1 (High)\n-87.63493248572952\nCHISOX BAR & GRILL\n2023-06-20\nPass\nCanvass\n320 W 35TH ST\n51. PLUMBING INSTALLED; PROPER BACKFLOW DEVICE...\nPOINT (-87.63493 41.83101)\n\n\n\n\n2 rows × 21 columns\n\n\n\nFor some reason, Socrata adds on these four always-null location columns on to geospatial exports. I’m going to remove them.\n\n\nCode\nlocation_cols = [\"location_state\", \"location_zip\", \"location_address\", \"location_city\"]\nprint(\"Rows with a non-null value in these location_xxx columns:\")\ndisplay(full_food_inspection_gdf[location_cols].notnull().sum())\nfull_food_inspection_gdf = full_food_inspection_gdf.drop(columns=location_cols)\n\n\nRows with a non-null value in these location_xxx columns:\n\n\nlocation_state      0\nlocation_zip        0\nlocation_address    0\nlocation_city       0\ndtype: int64\n\n\nThat column ordering is a bit chaotic, so I’ll reorder them (for readability).\n\n\nCode\ncol_order = [\n    \"inspection_id\", \"inspection_date\", \"dba_name\", \"aka_name\", \"license_\", \"facility_type\",\n    \"risk\", \"inspection_type\", \"results\", \"address\", \"city\", \"state\", \"zip\", \"violations\",\n    \"longitude\", \"latitude\", \"geometry\"\n]\nfull_food_inspection_gdf = full_food_inspection_gdf[col_order].copy()\n\n\nI also want to break this into batches based on the dates, so I need to cast the inspection_date to a datetime type.\n\n\nCode\nfull_food_inspection_gdf[\"inspection_date\"] = pd.to_datetime(\n    full_food_inspection_gdf[\"inspection_date\"]\n)\n\n\n\n\nCode\nfull_food_inspection_gdf.head(2)\n\n\n\n\n\n\n\n\n\ninspection_id\ninspection_date\ndba_name\naka_name\nlicense_\nfacility_type\nrisk\ninspection_type\nresults\naddress\ncity\nstate\nzip\nviolations\nlongitude\nlatitude\ngeometry\n\n\n\n\n0\n2577546\n2023-06-20\nCIAO RAGAZZI\nCIAO RAGAZZI\n2808408\nRestaurant\nRisk 1 (High)\nCanvass\nPass w/ Conditions\n5440 S NARRAGANSETT AVE\nCHICAGO\nIL\n60638\n10. ADEQUATE HANDWASHING SINKS PROPERLY SUPPLI...\n-87.78197456162096\n41.79300598548857\nPOINT (-87.78197 41.79301)\n\n\n1\n2577553\n2023-06-20\nCHISOX BAR & GRILL\nCHI SOX BAR & GRILL\n2078887\nRestaurant\nRisk 1 (High)\nCanvass\nPass\n320 W 35TH ST\nCHICAGO\nIL\n60616\n51. PLUMBING INSTALLED; PROPER BACKFLOW DEVICE...\n-87.63493248572952\n41.83100808816173\nPOINT (-87.63493 41.83101)\n\n\n\n\n\n\n\n\n\nCode\nmonth_start_dates = full_food_inspection_gdf[\"inspection_date\"].dt.to_period(\"M\").dt.to_timestamp().unique()\nmonth_start_dates.sort()\n\n\n\n\nCode\ndata_file_names = [dp.name for dp in POST_DATA_DIR.iterdir()]\ndata_file_names.sort()\ndata_file_names[0:5]\n\n\n['full_food_inspections.geojson']\n\n\n\n\nCode\nfor month_start_date in month_start_dates:\n    batch_period = pd.to_datetime(month_start_date).strftime(\"%Y_%m\")\n    batch_data = full_food_inspection_gdf.loc[\n        full_food_inspection_gdf[\"inspection_date\"].between(\n            left=month_start_date,\n            right=month_start_date + pd.DateOffset(months=1),\n            inclusive=\"left\")\n    ].copy()\n    batch_file_path = POST_DATA_DIR.joinpath(f\"food_inspection_batch_{batch_period}.parquet\")\n    if not batch_file_path.is_file():\n        batch_data.to_parquet(batch_file_path)\n\n\n\n\nCode\ndata_file_names = [dp.name for dp in POST_DATA_DIR.iterdir()]\ndata_file_names.sort()\ndata_file_names[0:5]\n\n\n['food_inspection_batch_2010_01.parquet',\n 'food_inspection_batch_2010_02.parquet',\n 'food_inspection_batch_2010_03.parquet',\n 'food_inspection_batch_2010_04.parquet',\n 'food_inspection_batch_2010_05.parquet']"
  },
  {
    "objectID": "posts/006_great_expectations_setup/data_monitoring_w_gx_1_setup.html#great-expectations-setup",
    "href": "posts/006_great_expectations_setup/data_monitoring_w_gx_1_setup.html#great-expectations-setup",
    "title": "Data Quality Monitoring with Great Expectations",
    "section": "Great Expectations Setup",
    "text": "Great Expectations Setup\nFirst, you’ll need to install the great_expectations. If you already have conda installed on your machine, you can easily set up a conda env just like the one used to run this notebook by: 1. copying the gx_env_environment.yml file in the same dir as this notebook file to your machine, 2. open a terminal and navigate to the dir with that new file, and 3. run command conda env create -f environment.yml"
  },
  {
    "objectID": "posts/006_great_expectations_setup/data_monitoring_w_gx_1_setup.html#create-or-load-great-expectations-data-context",
    "href": "posts/006_great_expectations_setup/data_monitoring_w_gx_1_setup.html#create-or-load-great-expectations-data-context",
    "title": "Data Quality Monitoring with Great Expectations",
    "section": "Create or Load Great Expectations Data Context",
    "text": "Create or Load Great Expectations Data Context\n\n\nCode\nimport great_expectations as gx\nfrom great_expectations.data_context import FileDataContext\n\ncontext = FileDataContext.create(project_root_dir=POST_DIR)"
  },
  {
    "objectID": "posts/006_great_expectations_setup/data_monitoring_w_gx_1_setup.html#create-a-datasource",
    "href": "posts/006_great_expectations_setup/data_monitoring_w_gx_1_setup.html#create-a-datasource",
    "title": "Data Quality Monitoring with Great Expectations",
    "section": "Create a Datasource",
    "text": "Create a Datasource\n\n\nCode\ndatasource_name = \"food_inspection_datasource\"\n\nif any(el[\"name\"] == datasource_name for el in context.list_datasources()):\n    print(f\"Datasource with name '{datasource_name}' found; loading now\")\n    datasource = context.get_datasource(datasource_name)\nelse:\n    print(f\"No Datasource with name '{datasource_name}' found; creating now\")\n    datasource = context.sources.add_pandas_filesystem(\n        name=datasource_name,\n        base_directory=POST_DATA_DIR\n    )\n\n\nDatasource with name 'food_inspection_datasource' found; loading now\n\n\n\n\nCode\n\n\n\nTrue\n\n\n\n\nCode\n[el for el in dir(datasource) if el.startswith(\"add_\")]\n\n\n['add_csv_asset',\n 'add_excel_asset',\n 'add_feather_asset',\n 'add_fwf_asset',\n 'add_hdf_asset',\n 'add_html_asset',\n 'add_json_asset',\n 'add_orc_asset',\n 'add_parquet_asset',\n 'add_pickle_asset',\n 'add_sas_asset',\n 'add_spss_asset',\n 'add_stata_asset',\n 'add_xml_asset']\n\n\n\n\nCode\ndata_asset_name = \"food_inspections_asset\"\n\nif data_asset_name not in datasource.get_asset_names():\n    print(f\"Creating data asset {data_asset_name}\")\n    data_asset = datasource.add_parquet_asset(\n        name=data_asset_name,\n        batching_regex = r\"food_inspection_batch_(?P&lt;year&gt;\\d{4})_(?P&lt;month&gt;\\d{2})\\.parquet\"\n    )\nelse:\n    data_asset = datasource.get_asset(data_asset_name)\n\n\nCreating data asset food_inspections_asset\n\n\nI’ll also sort these batches.\n\n\nCode\ndata_asset = data_asset.add_sorters([\"+year\", \"+month\"])\n\n\n\n\nCode\nbatch_request = data_asset.build_batch_request()\nbatches = data_asset.get_batch_list_from_batch_request(batch_request)"
  },
  {
    "objectID": "posts/006_great_expectations_setup/data_monitoring_w_gx_1_setup.html#using-the-profiler-to-create-basic-expectations",
    "href": "posts/006_great_expectations_setup/data_monitoring_w_gx_1_setup.html#using-the-profiler-to-create-basic-expectations",
    "title": "Data Quality Monitoring with Great Expectations",
    "section": "Using the profiler to create basic expectations",
    "text": "Using the profiler to create basic expectations\n\n\nCode\nexpectation_suite_name = \"food_inspections_suite\"\n\nexpectation_suite = context.add_or_update_expectation_suite(\n    expectation_suite_name=expectation_suite_name\n)\n\n\n\n\nCode\ndata_assistant_result = context.assistants.onboarding.run(\n    batch_request=batch_request,\n    exclude_column_names=[],\n)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCode\ndata_assistant_result.plot_expectations_and_metrics(exclude_column_names=[\"inspection_date\"])\n\n\n62 Expectations produced, 9 Expectation and Metric plots implemented\nUse DataAssistantResult.show_expectations_by_domain_type() or\nDataAssistantResult.show_expectations_by_expectation_type() to show all produced Expectations\n\n\n\n                \n                \n\n\n\n                \n            \n\n\n\n                \n            \n\n\n\n\n\n\n\n\n\n\nCode\nexpectation_suite = data_assistant_result.get_expectation_suite(\n    expectation_suite_name=expectation_suite_name\n)\n\n\n\n\nCode\n# expectation_suite.show_expectations_by_expectation_type()\n\n\n\n\nCode\nsaved_suite = context.add_or_update_expectation_suite(expectation_suite=expectation_suite)"
  },
  {
    "objectID": "posts/004_census_tiger_dev/TIGER_data_collector_dev.html",
    "href": "posts/004_census_tiger_dev/TIGER_data_collector_dev.html",
    "title": "Census TIGER Dataset Collector Dev",
    "section": "",
    "text": "Code\nimport textwrap\nfrom typing import Dict, List, Union, Optional\nfrom urllib.request import urlretrieve\n\nfrom bs4 import BeautifulSoup\nimport geopandas as gpd\nimport matplotlib.pyplot as plt\nimport pandas as pd\nimport requests\n\n\nThe US Census Bureau (USCB) runs a massive array of surveys of residents of the United States. After aggregating responses into demographic and/or geographic groups, the USCB publishes those aggregated datasets to its massive public data catalog. I’ve already developed some tooling to collect and ingest Census datasets into my personal data warehouse and analytics environment, but to map out geographic Census data or join in other geospatial datasets, I also need to collect and ingest spatial files describing the geographic boundaries of Census groupings. Fortunately, the USCB provides shapefiles with these geometries in their TIGER (Topologically Integrated Geographic Encoding and Referencing) data offerings, and in this notebook, I’ll synthesize the findings the experiments and research in my last notebook into a data model and data collection tools.\n\nReferences:\n\nTIGER Technical Docs\nTIGER Geographic Codes\n\nEach year, the USCB can add or alter geographic boundaries or connected data features, so they organize TIGER data offering into annual vintages, which can be downloaded either through the USCB web interface or from their file server interface (https://www2.census.gov/geo/tiger/).\nThe functions in the cell below aid in scraping the table off of any of pages linked in that file-tree interface.\n\n\nCode\ndef request_page(metadata_url: str) -&gt; requests.models.Response:\n    resp = requests.get(metadata_url)\n    if resp.status_code == 200:\n        return resp\n    else:\n        raise Exception(f\"Couldn't get page metadata for url {metadata_url}\")\n\ndef scrape_census_ftp_metadata_page(metadata_url: str) -&gt; pd.DataFrame:\n    resp = request_page(metadata_url=metadata_url)\n    soup = BeautifulSoup(resp.content, \"html.parser\")\n    table = soup.find(\"table\")\n    rows = table.find_all(\"tr\")\n    table_contents = []\n    for row in rows:\n        cols = row.find_all(\"td\")\n        cols = [col.text.strip() for col in cols]\n        table_contents.append(cols)\n    table_rows = [el for el in table_contents if len(el) &gt; 0]\n\n    metadata_df = pd.DataFrame(\n        [row[1:] for row in table_rows],\n        columns=[\"name\", \"last_modified\", \"size\", \"description\"],\n    )\n    metadata_df[\"last_modified\"] = pd.to_datetime(metadata_df[\"last_modified\"])\n    metadata_df[\"is_dir\"] = metadata_df[\"name\"].str.endswith(\"/\")\n    metadata_df[\"clean_name\"] = metadata_df[\"name\"].str.replace(\"/$\", \"\", regex=True)\n    metadata_df[\"is_file\"] = (~metadata_df[\"is_dir\"]) & (\n        metadata_df[\"clean_name\"] != \"Parent Directory\"\n    )\n    while metadata_url.strip().endswith(\"/\"):\n        metadata_url = metadata_url[:-1]\n    mask = metadata_df[\"is_file\"] | metadata_df[\"is_dir\"]\n    metadata_df = metadata_df.loc[mask].copy()\n    metadata_df[\"metadata_url\"] = (metadata_url + \"/\" + metadata_df[\"clean_name\"])\n    return metadata_df\n\n\nrequest_page() makes a GET request for the content at the URL metadata_url and returns the response (if successful, i.e. if the response has the “success” HTTP response status code). This function is only called by scrape_census_ftp_metadata_page(), but it’s separated out to make it easier to test scrape_census_ftp_metadata_page().\n\n\nCode\ndef request_page(metadata_url: str) -&gt; requests.models.Response:\n    resp = requests.get(metadata_url)\n    if resp.status_code == 200:\n        return resp\n    else:\n        raise Exception(f\"Couldn't get page metadata for url {metadata_url}\")\n\n\nscrape_census_ftp_metadata_page() gets the content from a given Census file-tree page (at URL metadata_url), parses it into a convenient and well structured format (a pandas DataFrame) with features that aid in filtering to desired rows.\n\n\nCode\nresp = request_page(metadata_url=\"https://www2.census.gov/geo/tiger/\")\nsoup = BeautifulSoup(resp.content, \"html.parser\")\ntable = soup.find(\"table\")\nrows = table.find_all(\"tr\")\nrows[0:7]\n\n\n[&lt;tr&gt;&lt;th valign=\"top\"&gt;&lt;img alt=\"[ICO]\" src=\"/icons/blank.gif\"/&gt;&lt;/th&gt;&lt;th&gt;&lt;a href=\"?C=N;O=D\"&gt;Name&lt;/a&gt;&lt;/th&gt;&lt;th&gt;&lt;a href=\"?C=M;O=A\"&gt;Last modified&lt;/a&gt;&lt;/th&gt;&lt;th&gt;&lt;a href=\"?C=S;O=A\"&gt;Size&lt;/a&gt;&lt;/th&gt;&lt;th&gt;&lt;a href=\"?C=D;O=A\"&gt;Description&lt;/a&gt;&lt;/th&gt;&lt;/tr&gt;,\n &lt;tr&gt;&lt;th colspan=\"5\"&gt;&lt;hr/&gt;&lt;/th&gt;&lt;/tr&gt;,\n &lt;tr&gt;&lt;td valign=\"top\"&gt;&lt;img alt=\"[PARENTDIR]\" src=\"/icons/back.gif\"/&gt;&lt;/td&gt;&lt;td&gt;&lt;a href=\"/geo/\"&gt;Parent Directory&lt;/a&gt;&lt;/td&gt;&lt;td&gt; &lt;/td&gt;&lt;td align=\"right\"&gt;  - &lt;/td&gt;&lt;td&gt; &lt;/td&gt;&lt;/tr&gt;,\n &lt;tr&gt;&lt;td valign=\"top\"&gt;&lt;img alt=\"[   ]\" src=\"/icons/layout.gif\"/&gt;&lt;/td&gt;&lt;td&gt;&lt;a href=\"Directory_Contents_ReadMe.pdf\"&gt;Directory_Contents_ReadMe.pdf&lt;/a&gt;&lt;/td&gt;&lt;td align=\"right\"&gt;2019-06-25 09:13  &lt;/td&gt;&lt;td align=\"right\"&gt;439K&lt;/td&gt;&lt;td&gt; &lt;/td&gt;&lt;/tr&gt;,\n &lt;tr&gt;&lt;td valign=\"top\"&gt;&lt;img alt=\"[DIR]\" src=\"/icons/folder.gif\"/&gt;&lt;/td&gt;&lt;td&gt;&lt;a href=\"GENZ2010/\"&gt;GENZ2010/&lt;/a&gt;&lt;/td&gt;&lt;td align=\"right\"&gt;2013-07-24 12:46  &lt;/td&gt;&lt;td align=\"right\"&gt;  - &lt;/td&gt;&lt;td&gt; &lt;/td&gt;&lt;/tr&gt;,\n &lt;tr&gt;&lt;td valign=\"top\"&gt;&lt;img alt=\"[DIR]\" src=\"/icons/folder.gif\"/&gt;&lt;/td&gt;&lt;td&gt;&lt;a href=\"GENZ2012/\"&gt;GENZ2012/&lt;/a&gt;&lt;/td&gt;&lt;td align=\"right\"&gt;2013-07-24 12:47  &lt;/td&gt;&lt;td align=\"right\"&gt;  - &lt;/td&gt;&lt;td&gt; &lt;/td&gt;&lt;/tr&gt;,\n &lt;tr&gt;&lt;td valign=\"top\"&gt;&lt;img alt=\"[DIR]\" src=\"/icons/folder.gif\"/&gt;&lt;/td&gt;&lt;td&gt;&lt;a href=\"GENZ2013/\"&gt;GENZ2013/&lt;/a&gt;&lt;/td&gt;&lt;td align=\"right\"&gt;2014-07-02 08:28  &lt;/td&gt;&lt;td align=\"right\"&gt;  - &lt;/td&gt;&lt;td&gt; &lt;/td&gt;&lt;/tr&gt;]\n\n\nThe cell above shows code that extracts HTML tr (table-row) elements extracted from the page (sorry for showing something so ugly!), and the cell below shows the (beautiful) end product of scrape_census_ftp_metadata_page().\n\n\nCode\nall_tiger_vintages_df = scrape_census_ftp_metadata_page(\n    metadata_url=\"https://www2.census.gov/geo/tiger/\"\n)\ndisplay(all_tiger_vintages_df.head(4))\n\n\n\n\n\n\n\n\n\nname\nlast_modified\nsize\ndescription\nis_dir\nclean_name\nis_file\nmetadata_url\n\n\n\n\n1\nDirectory_Contents_ReadMe.pdf\n2019-06-25 09:13:00\n439K\n\nFalse\nDirectory_Contents_ReadMe.pdf\nTrue\nhttps://www2.census.gov/geo/tiger/Directory_Co...\n\n\n2\nGENZ2010/\n2013-07-24 12:46:00\n-\n\nTrue\nGENZ2010\nFalse\nhttps://www2.census.gov/geo/tiger/GENZ2010\n\n\n3\nGENZ2012/\n2013-07-24 12:47:00\n-\n\nTrue\nGENZ2012\nFalse\nhttps://www2.census.gov/geo/tiger/GENZ2012\n\n\n4\nGENZ2013/\n2014-07-02 08:28:00\n-\n\nTrue\nGENZ2013\nFalse\nhttps://www2.census.gov/geo/tiger/GENZ2013\n\n\n\n\n\n\n\n\n\nCode\nprint(f\"Files on the top-level TIGER dataset page:                {all_tiger_vintages_df['is_file'].sum():&gt;3}\")\nprint(f\"TIGER data offerings on the top-level TIGER dataset page: {all_tiger_vintages_df['is_dir'].sum():&gt;3}\")\n\n\nFiles on the top-level TIGER dataset page:                  1\nTIGER data offerings on the top-level TIGER dataset page:  59\n\n\nThe GENZyyyy TIGER data offerings are interesting, but the real wealth of geospatial files can be found in the TIGER offerings with names matching the TIGERyyyy pattern, and I implemented the get_tiger_vintages_metadata() function to get the metadata for these offerings, or vintages.\n\n\nCode\ndef get_tiger_vintages_metadata() -&gt; pd.DataFrame:\n    all_tiger_vintages_df = scrape_census_ftp_metadata_page(\n        metadata_url=\"https://www2.census.gov/geo/tiger/\"\n    )\n    tiger_vintages_df = all_tiger_vintages_df.loc[\n        all_tiger_vintages_df[\"name\"].str.contains(\"^TIGER\\d{4}/\", regex=True)\n    ].copy()\n    tiger_vintages_df = tiger_vintages_df.sort_values(by=\"name\", ignore_index=True)\n    return tiger_vintages_df\n\ntiger_vintages_df = get_tiger_vintages_metadata()\nprint(f\"Available TIGER vintages:\")\nnames = textwrap.wrap(\", \".join(list(tiger_vintages_df[\"clean_name\"].values)), width=95)\nfor line in names:\n    print(f\"    {line}\")\ndisplay(tiger_vintages_df.head(3))\n\n\nAvailable TIGER vintages:\n    TIGER1992, TIGER1999, TIGER2002, TIGER2003, TIGER2008, TIGER2009, TIGER2010, TIGER2011,\n    TIGER2012, TIGER2013, TIGER2014, TIGER2015, TIGER2016, TIGER2017, TIGER2018, TIGER2019,\n    TIGER2020, TIGER2021, TIGER2022\n\n\n\n\n\n\n\n\n\nname\nlast_modified\nsize\ndescription\nis_dir\nclean_name\nis_file\nmetadata_url\n\n\n\n\n0\nTIGER1992/\n2011-12-19 07:56:00\n-\n\nTrue\nTIGER1992\nFalse\nhttps://www2.census.gov/geo/tiger/TIGER1992\n\n\n1\nTIGER1999/\n2012-02-13 11:19:00\n-\n\nTrue\nTIGER1999\nFalse\nhttps://www2.census.gov/geo/tiger/TIGER1999\n\n\n2\nTIGER2002/\n2015-05-05 18:37:00\n-\n\nTrue\nTIGER2002\nFalse\nhttps://www2.census.gov/geo/tiger/TIGER2002\n\n\n\n\n\n\n\nNow that we have tools for scraping the any page in the Census’s file server, we can use those tools to retrieve data on the geographic entities in a given TIGER vintage.\n\n\nCode\nclass TIGERCatalog:\n    def __init__(self):\n        self.dataset_vintages = get_tiger_vintages_metadata()\n\n    def get_vintage_metadata(self, year: str) -&gt; pd.DataFrame:\n        return self.dataset_vintages.loc[self.dataset_vintages[\"name\"] == f\"TIGER{year}/\"].copy()\n\nclass TIGERVintageCatalog:\n    def __init__(self, year: str, catalog: Optional[TIGERCatalog] = None):\n        self.year = str(year)\n        self.set_catalog(catalog=catalog)\n\n    def set_catalog(self, catalog: Optional[TIGERCatalog]) -&gt; None:\n        if catalog is None:\n            self.catalog = TIGERCatalog()\n        else:\n            self.catalog = catalog\n\n    @property\n    def vintage_metadata(self):\n        return self.catalog.get_vintage_metadata(year=self.year)\n\n    @property\n    def vintage_entities(self):\n        if len(self.vintage_metadata) == 1:\n            tiger_vintage_url = self.vintage_metadata[\"metadata_url\"].values[0]\n            return scrape_census_ftp_metadata_page(metadata_url=tiger_vintage_url)\n        else:\n            raise Exception(\n                f\"Failed to get unambiguous metadata (got {self.vintage_metadata})\"\n            )\n\n    def get_entity_metadata(self, entity_name: str) -&gt; pd.DataFrame:\n        return self.vintage_entities.loc[self.vintage_entities[\"clean_name\"] == entity_name].copy()\n\n    def print_entity_names(self):\n        entity_names = self.vintage_entities.loc[\n            self.vintage_entities[\"is_dir\"], \"clean_name\"\n        ].values\n        print(f\"TIGER Entity options for the {self.year} TIGER vintage:\")\n        for entity_name in entity_names:\n            print(f\"  - {entity_name}\")\n        print(f\"Entity count: {len(entity_names)}\")\n\n\n\n\nCode\ntiger_catalog = TIGERCatalog()\nvintage_entity_catalog = TIGERVintageCatalog(year=\"2022\", catalog=tiger_catalog)\nvintage_entity_catalog.print_entity_names()\n\n\nTIGER Entity options for the 2022 TIGER vintage:\n  - ADDR\n  - ADDRFEAT\n  - ADDRFN\n  - AIANNH\n  - AITSN\n  - ANRC\n  - AREALM\n  - AREAWATER\n  - BG\n  - CD\n  - COASTLINE\n  - CONCITY\n  - COUNTY\n  - COUSUB\n  - EDGES\n  - ELSD\n  - ESTATE\n  - FACES\n  - FACESAH\n  - FACESAL\n  - FACESMIL\n  - FEATNAMES\n  - LINEARWATER\n  - MIL\n  - PLACE\n  - POINTLM\n  - PRIMARYROADS\n  - PRISECROADS\n  - PUMA\n  - RAILS\n  - ROADS\n  - SCSD\n  - SDADM\n  - SLDL\n  - SLDU\n  - STATE\n  - SUBBARRIO\n  - TABBLOCK20\n  - TBG\n  - TRACT\n  - TTRACT\n  - UAC\n  - UNSD\n  - ZCTA520\nEntity count: 44\n\n\nLet’s examine Census Tracts.\nI’ll need some tooling to collect information on a given entity in a given TIGER vintage. I know this object will need data that’s in the relevant TIGERVintageCatalog instance, so I’ll make that an attribute of the entity vintage class. I could require that the use passes in a TIGERVintageCatalog instance (which would reduce the number of calls to the same Census resource in the usecase where a user is interactively working using these classes), but ultimately I’m going to build out pipelines that only collect one TIGER entity at a from a given vintage (as I don’t need most of the available entities listed above).\n\n\nCode\nclass TIGERGeographicEntityVintage:\n    def __init__(self, entity_name: str, year: str, catalog: Optional[TIGERCatalog] = None):\n        self.entity_name = entity_name\n        self.year = str(year)\n        self.vintage_catalog = TIGERVintageCatalog(year=year, catalog=catalog)\n        self.entity_metadata = self.vintage_catalog.get_entity_metadata(entity_name=self.entity_name)\n\n    @property\n    def entity_url(self):\n        return self.entity_metadata[\"metadata_url\"].values[0]\n\ntiger_tract22_obj = TIGERGeographicEntityVintage(entity_name=\"TRACT\", year=\"2022\", catalog=tiger_catalog)\ndisplay(tiger_tract22_obj.entity_metadata)\nentity_df = scrape_census_ftp_metadata_page(metadata_url=tiger_tract22_obj.entity_url)\nprint(entity_df.shape)\ndisplay(entity_df.head(2))\n\n\n\n\n\n\n\n\n\nname\nlast_modified\nsize\ndescription\nis_dir\nclean_name\nis_file\nmetadata_url\n\n\n\n\n41\nTRACT/\n2022-09-30 22:39:00\n-\n\nTrue\nTRACT\nFalse\nhttps://www2.census.gov/geo/tiger/TIGER2022/TRACT\n\n\n\n\n\n\n\n(56, 8)\n\n\n\n\n\n\n\n\n\nname\nlast_modified\nsize\ndescription\nis_dir\nclean_name\nis_file\nmetadata_url\n\n\n\n\n1\ntl_2022_01_tract.zip\n2022-10-31 19:42:00\n11M\n\nFalse\ntl_2022_01_tract.zip\nTrue\nhttps://www2.census.gov/geo/tiger/TIGER2022/TR...\n\n\n2\ntl_2022_02_tract.zip\n2022-10-31 19:42:00\n3.0M\n\nFalse\ntl_2022_02_tract.zip\nTrue\nhttps://www2.census.gov/geo/tiger/TIGER2022/TR...\n\n\n\n\n\n\n\nLooking at the names, I see that the Census groups tracts by state FIPS code (Federal Information Processing Series code). I know the FIPS code for Illinois is “17” (you can review the FIPS codes for other geographic entities here). I also know I don’t always want to pull data for all states, so I need to add a method that allows the user to filter the entity files. Also, I should build in the step of getting the entity files metadata.\n\n\nCode\nclass TIGERGeographicEntityVintage:\n    def __init__(self, entity_name: str, year: str, catalog: Optional[TIGERCatalog] = None):\n        self.entity_name = entity_name\n        self.year = str(year)\n        self.vintage_catalog = TIGERVintageCatalog(year=year, catalog=catalog)\n        self.entity_metadata = self.vintage_catalog.get_entity_metadata(entity_name=self.entity_name)\n\n    @property\n    def entity_url(self):\n        return self.entity_metadata[\"metadata_url\"].values[0]\n\n    @property\n    def entity_files_metadata(self):\n        return scrape_census_ftp_metadata_page(metadata_url=self.entity_url)\n\n    def get_entity_file_metadata(self, filter_str: str) -&gt; pd.DataFrame:\n        return self.entity_files_metadata.loc[self.entity_files_metadata[\"name\"].str.contains(filter_str)].copy()\n\ntiger_tract22_obj = TIGERGeographicEntityVintage(entity_name=\"TRACT\", year=\"2022\", catalog=tiger_catalog)\nil_tracts22_metadata = tiger_tract22_obj.get_entity_file_metadata(filter_str=\"_17_\")\ndisplay(il_tracts22_metadata)\nprint(f\"IL Tracts archive download Url: {il_tracts22_metadata['metadata_url'].values[0]}\")\n\n\n\n\n\n\n\n\n\nname\nlast_modified\nsize\ndescription\nis_dir\nclean_name\nis_file\nmetadata_url\n\n\n\n\n14\ntl_2022_17_tract.zip\n2022-10-31 19:43:00\n9.5M\n\nFalse\ntl_2022_17_tract.zip\nTrue\nhttps://www2.census.gov/geo/tiger/TIGER2022/TR...\n\n\n\n\n\n\n\nIL Tracts archive download Url: https://www2.census.gov/geo/tiger/TIGER2022/TRACT/tl_2022_17_tract.zip\n\n\nGeopandas provides some extremely convenient functionality for loading data. I can provide the URL to a zipped archive (of geospatial files) to geopandas’ read_file() function and it handles the network request and unzipping for me.\n\n\nCode\nil_tracts_gdf = gpd.read_file(il_tracts22_metadata[\"metadata_url\"].values[0])\n\n\n\n\nCode\nprint(il_tracts_gdf.shape)\nil_tracts_gdf.head(2)\n\n\n(3265, 13)\n\n\n\n\n\n\n\n\n\nSTATEFP\nCOUNTYFP\nTRACTCE\nGEOID\nNAME\nNAMELSAD\nMTFCC\nFUNCSTAT\nALAND\nAWATER\nINTPTLAT\nINTPTLON\ngeometry\n\n\n\n\n0\n17\n019\n010701\n17019010701\n107.01\nCensus Tract 107.01\nG5020\nS\n5266000\n30553\n+40.1150269\n-088.0329549\nPOLYGON ((-88.05240 40.11923, -88.05238 40.119...\n\n\n1\n17\n019\n005902\n17019005902\n59.02\nCensus Tract 59.02\nG5020\nS\n962402\n4892\n+40.1087344\n-088.2247204\nPOLYGON ((-88.22891 40.11271, -88.22882 40.112...\n\n\n\n\n\n\n\nAnd now I have plottable, spatially-joinable geospatial data ready to ingest into a data warehouse table or to plot out.\n\n\nCode\nfig_width = 14\n\nfig, ax = plt.subplots(figsize=(fig_width, fig_width))\nax = il_tracts_gdf.plot(facecolor=\"none\", edgecolor=\"black\", linewidth=0.015 * fig_width, ax=ax)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/002_notebook_test/jupyter_quarto.html",
    "href": "posts/002_notebook_test/jupyter_quarto.html",
    "title": "Working with Notebooks",
    "section": "",
    "text": "Code\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nr = np.arange(0, 4, 0.01)\ntheta = 2 * np.pi * r\nfig, ax = plt.subplots(\n  subplot_kw = {'projection': 'polar'} \n)\nax.plot(theta, r)\nax.set_rticks([0.5, 1, 1.5, 2])\nax.grid(True)\nplt.show()\n\n\n\n\n\nFigure 1: A line plot on a polar axis\n\n\n\n\n\nDevelop a notebook\nRender that notebook via\n(quarto_env) user@hostname:~/...$ quarto render notebook_name.ipynb\nPreview your document via\n(quarto_env) user@hostname:~/...$ quarto preview quarto_blog/\nNote: Make sure draft: false in your document, or it won’t render.\n\n\n\n\n\n\n\n\n\n\nNo matching items"
  }
]