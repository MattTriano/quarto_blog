[
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "Matt Triano is a Data Scientist at the University of Chicago’s Urban Labs. When not innovating on data platforms, Matt enjoys exploring new tech, open source development, and hardware hacking.\n\n\nDePaul University | Chicago, IL M.S. in Computer Science | Sept 2013 - Mar 2018\nPurdue University | West Lafayette, IN B.S. in Physics | Aug 2006 - Dec 2010"
  },
  {
    "objectID": "about.html#education",
    "href": "about.html#education",
    "title": "About",
    "section": "",
    "text": "DePaul University | Chicago, IL M.S. in Computer Science | Sept 2013 - Mar 2018\nPurdue University | West Lafayette, IN B.S. in Physics | Aug 2006 - Dec 2010"
  },
  {
    "objectID": "posts/004_census_tiger_dev/TIGER_data_collector_dev.html",
    "href": "posts/004_census_tiger_dev/TIGER_data_collector_dev.html",
    "title": "Census TIGER Dataset Collector Dev",
    "section": "",
    "text": "Code\nimport textwrap\nfrom typing import Dict, List, Union, Optional\nfrom urllib.request import urlretrieve\n\nfrom bs4 import BeautifulSoup\nimport geopandas as gpd\nimport matplotlib.pyplot as plt\nimport pandas as pd\nimport requests\n\n\nThe US Census Bureau (USCB) runs a massive array of surveys of residents of the United States. After aggregating responses into demographic and/or geographic groups, the USCB publishes those aggregated datasets to its massive public data catalog. I’ve already developed some tooling to collect and ingest Census datasets into my personal data warehouse and analytics environment, but to map out geographic Census data or join in other geospatial datasets, I also need to collect and ingest spatial files describing the geographic boundaries of Census groupings. Fortunately, the USCB provides shapefiles with these geometries in their TIGER (Topologically Integrated Geographic Encoding and Referencing) data offerings, and in this notebook, I’ll synthesize the findings the experiments and research in my last notebook into a data model and data collection tools.\n\nReferences:\n\nTIGER Technical Docs\nTIGER Geographic Codes\n\nEach year, the USCB can add or alter geographic boundaries or connected data features, so they organize TIGER data offering into annual vintages, which can be downloaded either through the USCB web interface or from their file server interface (https://www2.census.gov/geo/tiger/).\nThe functions in the cell below aid in scraping the table off of any of pages linked in that file-tree interface.\n\n\nCode\ndef request_page(metadata_url: str) -&gt; requests.models.Response:\n    resp = requests.get(metadata_url)\n    if resp.status_code == 200:\n        return resp\n    else:\n        raise Exception(f\"Couldn't get page metadata for url {metadata_url}\")\n\ndef scrape_census_ftp_metadata_page(metadata_url: str) -&gt; pd.DataFrame:\n    resp = request_page(metadata_url=metadata_url)\n    soup = BeautifulSoup(resp.content, \"html.parser\")\n    table = soup.find(\"table\")\n    rows = table.find_all(\"tr\")\n    table_contents = []\n    for row in rows:\n        cols = row.find_all(\"td\")\n        cols = [col.text.strip() for col in cols]\n        table_contents.append(cols)\n    table_rows = [el for el in table_contents if len(el) &gt; 0]\n\n    metadata_df = pd.DataFrame(\n        [row[1:] for row in table_rows],\n        columns=[\"name\", \"last_modified\", \"size\", \"description\"],\n    )\n    metadata_df[\"last_modified\"] = pd.to_datetime(metadata_df[\"last_modified\"])\n    metadata_df[\"is_dir\"] = metadata_df[\"name\"].str.endswith(\"/\")\n    metadata_df[\"clean_name\"] = metadata_df[\"name\"].str.replace(\"/$\", \"\", regex=True)\n    metadata_df[\"is_file\"] = (~metadata_df[\"is_dir\"]) & (\n        metadata_df[\"clean_name\"] != \"Parent Directory\"\n    )\n    while metadata_url.strip().endswith(\"/\"):\n        metadata_url = metadata_url[:-1]\n    mask = metadata_df[\"is_file\"] | metadata_df[\"is_dir\"]\n    metadata_df = metadata_df.loc[mask].copy()\n    metadata_df[\"metadata_url\"] = (metadata_url + \"/\" + metadata_df[\"clean_name\"])\n    return metadata_df\n\n\nrequest_page() makes a GET request for the content at the URL metadata_url and returns the response (if successful, i.e. if the response has the “success” HTTP response status code). This function is only called by scrape_census_ftp_metadata_page(), but it’s separated out to make it easier to test scrape_census_ftp_metadata_page().\n\n\nCode\ndef request_page(metadata_url: str) -&gt; requests.models.Response:\n    resp = requests.get(metadata_url)\n    if resp.status_code == 200:\n        return resp\n    else:\n        raise Exception(f\"Couldn't get page metadata for url {metadata_url}\")\n\n\nscrape_census_ftp_metadata_page() gets the content from a given Census file-tree page (at URL metadata_url), parses it into a convenient and well structured format (a pandas DataFrame) with features that aid in filtering to desired rows.\n\n\nCode\nresp = request_page(metadata_url=\"https://www2.census.gov/geo/tiger/\")\nsoup = BeautifulSoup(resp.content, \"html.parser\")\ntable = soup.find(\"table\")\nrows = table.find_all(\"tr\")\nrows[0:7]\n\n\n[&lt;tr&gt;&lt;th valign=\"top\"&gt;&lt;img alt=\"[ICO]\" src=\"/icons/blank.gif\"/&gt;&lt;/th&gt;&lt;th&gt;&lt;a href=\"?C=N;O=D\"&gt;Name&lt;/a&gt;&lt;/th&gt;&lt;th&gt;&lt;a href=\"?C=M;O=A\"&gt;Last modified&lt;/a&gt;&lt;/th&gt;&lt;th&gt;&lt;a href=\"?C=S;O=A\"&gt;Size&lt;/a&gt;&lt;/th&gt;&lt;th&gt;&lt;a href=\"?C=D;O=A\"&gt;Description&lt;/a&gt;&lt;/th&gt;&lt;/tr&gt;,\n &lt;tr&gt;&lt;th colspan=\"5\"&gt;&lt;hr/&gt;&lt;/th&gt;&lt;/tr&gt;,\n &lt;tr&gt;&lt;td valign=\"top\"&gt;&lt;img alt=\"[PARENTDIR]\" src=\"/icons/back.gif\"/&gt;&lt;/td&gt;&lt;td&gt;&lt;a href=\"/geo/\"&gt;Parent Directory&lt;/a&gt;&lt;/td&gt;&lt;td&gt; &lt;/td&gt;&lt;td align=\"right\"&gt;  - &lt;/td&gt;&lt;td&gt; &lt;/td&gt;&lt;/tr&gt;,\n &lt;tr&gt;&lt;td valign=\"top\"&gt;&lt;img alt=\"[   ]\" src=\"/icons/layout.gif\"/&gt;&lt;/td&gt;&lt;td&gt;&lt;a href=\"Directory_Contents_ReadMe.pdf\"&gt;Directory_Contents_ReadMe.pdf&lt;/a&gt;&lt;/td&gt;&lt;td align=\"right\"&gt;2019-06-25 09:13  &lt;/td&gt;&lt;td align=\"right\"&gt;439K&lt;/td&gt;&lt;td&gt; &lt;/td&gt;&lt;/tr&gt;,\n &lt;tr&gt;&lt;td valign=\"top\"&gt;&lt;img alt=\"[DIR]\" src=\"/icons/folder.gif\"/&gt;&lt;/td&gt;&lt;td&gt;&lt;a href=\"GENZ2010/\"&gt;GENZ2010/&lt;/a&gt;&lt;/td&gt;&lt;td align=\"right\"&gt;2013-07-24 12:46  &lt;/td&gt;&lt;td align=\"right\"&gt;  - &lt;/td&gt;&lt;td&gt; &lt;/td&gt;&lt;/tr&gt;,\n &lt;tr&gt;&lt;td valign=\"top\"&gt;&lt;img alt=\"[DIR]\" src=\"/icons/folder.gif\"/&gt;&lt;/td&gt;&lt;td&gt;&lt;a href=\"GENZ2012/\"&gt;GENZ2012/&lt;/a&gt;&lt;/td&gt;&lt;td align=\"right\"&gt;2013-07-24 12:47  &lt;/td&gt;&lt;td align=\"right\"&gt;  - &lt;/td&gt;&lt;td&gt; &lt;/td&gt;&lt;/tr&gt;,\n &lt;tr&gt;&lt;td valign=\"top\"&gt;&lt;img alt=\"[DIR]\" src=\"/icons/folder.gif\"/&gt;&lt;/td&gt;&lt;td&gt;&lt;a href=\"GENZ2013/\"&gt;GENZ2013/&lt;/a&gt;&lt;/td&gt;&lt;td align=\"right\"&gt;2014-07-02 08:28  &lt;/td&gt;&lt;td align=\"right\"&gt;  - &lt;/td&gt;&lt;td&gt; &lt;/td&gt;&lt;/tr&gt;]\n\n\nThe cell above shows code that extracts HTML tr (table-row) elements extracted from the page (sorry for showing something so ugly!), and the cell below shows the (beautiful) end product of scrape_census_ftp_metadata_page().\n\n\nCode\nall_tiger_vintages_df = scrape_census_ftp_metadata_page(\n    metadata_url=\"https://www2.census.gov/geo/tiger/\"\n)\ndisplay(all_tiger_vintages_df.head(4))\n\n\n\n\n\n\n\n\n\nname\nlast_modified\nsize\ndescription\nis_dir\nclean_name\nis_file\nmetadata_url\n\n\n\n\n1\nDirectory_Contents_ReadMe.pdf\n2019-06-25 09:13:00\n439K\n\nFalse\nDirectory_Contents_ReadMe.pdf\nTrue\nhttps://www2.census.gov/geo/tiger/Directory_Co...\n\n\n2\nGENZ2010/\n2013-07-24 12:46:00\n-\n\nTrue\nGENZ2010\nFalse\nhttps://www2.census.gov/geo/tiger/GENZ2010\n\n\n3\nGENZ2012/\n2013-07-24 12:47:00\n-\n\nTrue\nGENZ2012\nFalse\nhttps://www2.census.gov/geo/tiger/GENZ2012\n\n\n4\nGENZ2013/\n2014-07-02 08:28:00\n-\n\nTrue\nGENZ2013\nFalse\nhttps://www2.census.gov/geo/tiger/GENZ2013\n\n\n\n\n\n\n\n\n\nCode\nprint(f\"Files on the top-level TIGER dataset page:                {all_tiger_vintages_df['is_file'].sum():&gt;3}\")\nprint(f\"TIGER data offerings on the top-level TIGER dataset page: {all_tiger_vintages_df['is_dir'].sum():&gt;3}\")\n\n\nFiles on the top-level TIGER dataset page:                  1\nTIGER data offerings on the top-level TIGER dataset page:  59\n\n\nThe GENZyyyy TIGER data offerings are interesting, but the real wealth of geospatial files can be found in the TIGER offerings with names matching the TIGERyyyy pattern, and I implemented the get_tiger_vintages_metadata() function to get the metadata for these offerings, or vintages.\n\n\nCode\ndef get_tiger_vintages_metadata() -&gt; pd.DataFrame:\n    all_tiger_vintages_df = scrape_census_ftp_metadata_page(\n        metadata_url=\"https://www2.census.gov/geo/tiger/\"\n    )\n    tiger_vintages_df = all_tiger_vintages_df.loc[\n        all_tiger_vintages_df[\"name\"].str.contains(\"^TIGER\\d{4}/\", regex=True)\n    ].copy()\n    tiger_vintages_df = tiger_vintages_df.sort_values(by=\"name\", ignore_index=True)\n    return tiger_vintages_df\n\ntiger_vintages_df = get_tiger_vintages_metadata()\nprint(f\"Available TIGER vintages:\")\nnames = textwrap.wrap(\", \".join(list(tiger_vintages_df[\"clean_name\"].values)), width=95)\nfor line in names:\n    print(f\"    {line}\")\ndisplay(tiger_vintages_df.head(3))\n\n\nAvailable TIGER vintages:\n    TIGER1992, TIGER1999, TIGER2002, TIGER2003, TIGER2008, TIGER2009, TIGER2010, TIGER2011,\n    TIGER2012, TIGER2013, TIGER2014, TIGER2015, TIGER2016, TIGER2017, TIGER2018, TIGER2019,\n    TIGER2020, TIGER2021, TIGER2022\n\n\n\n\n\n\n\n\n\nname\nlast_modified\nsize\ndescription\nis_dir\nclean_name\nis_file\nmetadata_url\n\n\n\n\n0\nTIGER1992/\n2011-12-19 07:56:00\n-\n\nTrue\nTIGER1992\nFalse\nhttps://www2.census.gov/geo/tiger/TIGER1992\n\n\n1\nTIGER1999/\n2012-02-13 11:19:00\n-\n\nTrue\nTIGER1999\nFalse\nhttps://www2.census.gov/geo/tiger/TIGER1999\n\n\n2\nTIGER2002/\n2015-05-05 18:37:00\n-\n\nTrue\nTIGER2002\nFalse\nhttps://www2.census.gov/geo/tiger/TIGER2002\n\n\n\n\n\n\n\nNow that we have tools for scraping the any page in the Census’s file server, we can use those tools to retrieve data on the geographic entities in a given TIGER vintage.\n\n\nCode\nclass TIGERCatalog:\n    def __init__(self):\n        self.dataset_vintages = get_tiger_vintages_metadata()\n\n    def get_vintage_metadata(self, year: str) -&gt; pd.DataFrame:\n        return self.dataset_vintages.loc[self.dataset_vintages[\"name\"] == f\"TIGER{year}/\"].copy()\n\nclass TIGERVintageCatalog:\n    def __init__(self, year: str, catalog: Optional[TIGERCatalog] = None):\n        self.year = str(year)\n        self.set_catalog(catalog=catalog)\n\n    def set_catalog(self, catalog: Optional[TIGERCatalog]) -&gt; None:\n        if catalog is None:\n            self.catalog = TIGERCatalog()\n        else:\n            self.catalog = catalog\n\n    @property\n    def vintage_metadata(self):\n        return self.catalog.get_vintage_metadata(year=self.year)\n\n    @property\n    def vintage_entities(self):\n        if len(self.vintage_metadata) == 1:\n            tiger_vintage_url = self.vintage_metadata[\"metadata_url\"].values[0]\n            return scrape_census_ftp_metadata_page(metadata_url=tiger_vintage_url)\n        else:\n            raise Exception(\n                f\"Failed to get unambiguous metadata (got {self.vintage_metadata})\"\n            )\n\n    def get_entity_metadata(self, entity_name: str) -&gt; pd.DataFrame:\n        return self.vintage_entities.loc[self.vintage_entities[\"clean_name\"] == entity_name].copy()\n\n    def print_entity_names(self):\n        entity_names = self.vintage_entities.loc[\n            self.vintage_entities[\"is_dir\"], \"clean_name\"\n        ].values\n        print(f\"TIGER Entity options for the {self.year} TIGER vintage:\")\n        for entity_name in entity_names:\n            print(f\"  - {entity_name}\")\n        print(f\"Entity count: {len(entity_names)}\")\n\n\n\n\nCode\ntiger_catalog = TIGERCatalog()\nvintage_entity_catalog = TIGERVintageCatalog(year=\"2022\", catalog=tiger_catalog)\nvintage_entity_catalog.print_entity_names()\n\n\nTIGER Entity options for the 2022 TIGER vintage:\n  - ADDR\n  - ADDRFEAT\n  - ADDRFN\n  - AIANNH\n  - AITSN\n  - ANRC\n  - AREALM\n  - AREAWATER\n  - BG\n  - CD\n  - COASTLINE\n  - CONCITY\n  - COUNTY\n  - COUSUB\n  - EDGES\n  - ELSD\n  - ESTATE\n  - FACES\n  - FACESAH\n  - FACESAL\n  - FACESMIL\n  - FEATNAMES\n  - LINEARWATER\n  - MIL\n  - PLACE\n  - POINTLM\n  - PRIMARYROADS\n  - PRISECROADS\n  - PUMA\n  - RAILS\n  - ROADS\n  - SCSD\n  - SDADM\n  - SLDL\n  - SLDU\n  - STATE\n  - SUBBARRIO\n  - TABBLOCK20\n  - TBG\n  - TRACT\n  - TTRACT\n  - UAC\n  - UNSD\n  - ZCTA520\nEntity count: 44\n\n\nLet’s examine Census Tracts.\nI’ll need some tooling to collect information on a given entity in a given TIGER vintage. I know this object will need data that’s in the relevant TIGERVintageCatalog instance, so I’ll make that an attribute of the entity vintage class. I could require that the use passes in a TIGERVintageCatalog instance (which would reduce the number of calls to the same Census resource in the usecase where a user is interactively working using these classes), but ultimately I’m going to build out pipelines that only collect one TIGER entity at a from a given vintage (as I don’t need most of the available entities listed above).\n\n\nCode\nclass TIGERGeographicEntityVintage:\n    def __init__(self, entity_name: str, year: str, catalog: Optional[TIGERCatalog] = None):\n        self.entity_name = entity_name\n        self.year = str(year)\n        self.vintage_catalog = TIGERVintageCatalog(year=year, catalog=catalog)\n        self.entity_metadata = self.vintage_catalog.get_entity_metadata(entity_name=self.entity_name)\n\n    @property\n    def entity_url(self):\n        return self.entity_metadata[\"metadata_url\"].values[0]\n\ntiger_tract22_obj = TIGERGeographicEntityVintage(entity_name=\"TRACT\", year=\"2022\", catalog=tiger_catalog)\ndisplay(tiger_tract22_obj.entity_metadata)\nentity_df = scrape_census_ftp_metadata_page(metadata_url=tiger_tract22_obj.entity_url)\nprint(entity_df.shape)\ndisplay(entity_df.head(2))\n\n\n\n\n\n\n\n\n\nname\nlast_modified\nsize\ndescription\nis_dir\nclean_name\nis_file\nmetadata_url\n\n\n\n\n41\nTRACT/\n2022-09-30 22:39:00\n-\n\nTrue\nTRACT\nFalse\nhttps://www2.census.gov/geo/tiger/TIGER2022/TRACT\n\n\n\n\n\n\n\n(56, 8)\n\n\n\n\n\n\n\n\n\nname\nlast_modified\nsize\ndescription\nis_dir\nclean_name\nis_file\nmetadata_url\n\n\n\n\n1\ntl_2022_01_tract.zip\n2022-10-31 19:42:00\n11M\n\nFalse\ntl_2022_01_tract.zip\nTrue\nhttps://www2.census.gov/geo/tiger/TIGER2022/TR...\n\n\n2\ntl_2022_02_tract.zip\n2022-10-31 19:42:00\n3.0M\n\nFalse\ntl_2022_02_tract.zip\nTrue\nhttps://www2.census.gov/geo/tiger/TIGER2022/TR...\n\n\n\n\n\n\n\nLooking at the names, I see that the Census groups tracts by state FIPS code (Federal Information Processing Series code). I know the FIPS code for Illinois is “17” (you can review the FIPS codes for other geographic entities here). I also know I don’t always want to pull data for all states, so I need to add a method that allows the user to filter the entity files. Also, I should build in the step of getting the entity files metadata.\n\n\nCode\nclass TIGERGeographicEntityVintage:\n    def __init__(self, entity_name: str, year: str, catalog: Optional[TIGERCatalog] = None):\n        self.entity_name = entity_name\n        self.year = str(year)\n        self.vintage_catalog = TIGERVintageCatalog(year=year, catalog=catalog)\n        self.entity_metadata = self.vintage_catalog.get_entity_metadata(entity_name=self.entity_name)\n\n    @property\n    def entity_url(self):\n        return self.entity_metadata[\"metadata_url\"].values[0]\n\n    @property\n    def entity_files_metadata(self):\n        return scrape_census_ftp_metadata_page(metadata_url=self.entity_url)\n\n    def get_entity_file_metadata(self, filter_str: str) -&gt; pd.DataFrame:\n        return self.entity_files_metadata.loc[self.entity_files_metadata[\"name\"].str.contains(filter_str)].copy()\n\ntiger_tract22_obj = TIGERGeographicEntityVintage(entity_name=\"TRACT\", year=\"2022\", catalog=tiger_catalog)\nil_tracts22_metadata = tiger_tract22_obj.get_entity_file_metadata(filter_str=\"_17_\")\ndisplay(il_tracts22_metadata)\nprint(f\"IL Tracts archive download Url: {il_tracts22_metadata['metadata_url'].values[0]}\")\n\n\n\n\n\n\n\n\n\nname\nlast_modified\nsize\ndescription\nis_dir\nclean_name\nis_file\nmetadata_url\n\n\n\n\n14\ntl_2022_17_tract.zip\n2022-10-31 19:43:00\n9.5M\n\nFalse\ntl_2022_17_tract.zip\nTrue\nhttps://www2.census.gov/geo/tiger/TIGER2022/TR...\n\n\n\n\n\n\n\nIL Tracts archive download Url: https://www2.census.gov/geo/tiger/TIGER2022/TRACT/tl_2022_17_tract.zip\n\n\nGeopandas provides some extremely convenient functionality for loading data. I can provide the URL to a zipped archive (of geospatial files) to geopandas’ read_file() function and it handles the network request and unzipping for me.\n\n\nCode\nil_tracts_gdf = gpd.read_file(il_tracts22_metadata[\"metadata_url\"].values[0])\n\n\n\n\nCode\nprint(il_tracts_gdf.shape)\nil_tracts_gdf.head(2)\n\n\n(3265, 13)\n\n\n\n\n\n\n\n\n\nSTATEFP\nCOUNTYFP\nTRACTCE\nGEOID\nNAME\nNAMELSAD\nMTFCC\nFUNCSTAT\nALAND\nAWATER\nINTPTLAT\nINTPTLON\ngeometry\n\n\n\n\n0\n17\n019\n010701\n17019010701\n107.01\nCensus Tract 107.01\nG5020\nS\n5266000\n30553\n+40.1150269\n-088.0329549\nPOLYGON ((-88.05240 40.11923, -88.05238 40.119...\n\n\n1\n17\n019\n005902\n17019005902\n59.02\nCensus Tract 59.02\nG5020\nS\n962402\n4892\n+40.1087344\n-088.2247204\nPOLYGON ((-88.22891 40.11271, -88.22882 40.112...\n\n\n\n\n\n\n\nAnd now I have plottable, spatially-joinable geospatial data ready to ingest into a data warehouse table or to plot out.\n\n\nCode\nfig_width = 14\n\nfig, ax = plt.subplots(figsize=(fig_width, fig_width))\nax = il_tracts_gdf.plot(facecolor=\"none\", edgecolor=\"black\", linewidth=0.015 * fig_width, ax=ax)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/000_setting_up_miniconda/Setting_up_Miniconda.html",
    "href": "posts/000_setting_up_miniconda/Setting_up_Miniconda.html",
    "title": "Setting up Conda",
    "section": "",
    "text": "conda is a language-agnostic package manager and environment management system. conda’s environment management functionality makes it possible for a user to easily switch between environments (where an environment consists of the hardware and software used to execute code) and makes it possible to export a specification of that environment that can be used to reproduce that environment on another system.\nIn this post, I’ll show my opinionated conda installation and configuration process.\n\n\nconda is primarily installable via two distributions; the Anaconda distribution, which includes the conda executable along with over 700 additional conda packages from the Anaconda repository, and the Miniconda distribution, which consists of the conda executable with the minimal number of packages needed for conda to run. Install a miniconda distribution."
  },
  {
    "objectID": "posts/000_setting_up_miniconda/Setting_up_Miniconda.html#conda-installation",
    "href": "posts/000_setting_up_miniconda/Setting_up_Miniconda.html#conda-installation",
    "title": "Setting up Conda",
    "section": "",
    "text": "conda is primarily installable via two distributions; the Anaconda distribution, which includes the conda executable along with over 700 additional conda packages from the Anaconda repository, and the Miniconda distribution, which consists of the conda executable with the minimal number of packages needed for conda to run. Install a miniconda distribution."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Experiments, how-tos, and gifts to my future self",
    "section": "",
    "text": "Order By\n       Default\n         \n          Title\n        \n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n         \n          Author\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n  \n\n\n\n\nHow to Manually Reproduce a Conda Env\n\n\n\n\n\n\n\nconda\n\n\ntutorial\n\n\nhowto\n\n\n\n\nHow to reverse engineer an old env and rerun old work\n\n\n\n\n\n\nJul 13, 2023\n\n\nMatt Triano\n\n\n\n\n\n\n  \n\n\n\n\nMidjourney Experiments\n\n\n\n\n\n\n\ngenerative AI\n\n\nmidjourney\n\n\nprompt eng\n\n\n\n\nHow to get started playing with Midjourney\n\n\n\n\n\n\nJul 1, 2023\n\n\nMatt Triano\n\n\n\n\n\n\n  \n\n\n\n\nData Quality Monitoring with Great Expectations\n\n\n\n\n\n\n\ngreat_expectations\n\n\ntutorial\n\n\nlong\n\n\n\n\nA full tutorial of a basic workflow\n\n\n\n\n\n\nJun 23, 2023\n\n\nMatt Triano\n\n\n\n\n\n\n  \n\n\n\n\nCensus TIGER Dataset Collector Dev\n\n\n\n\n\nDocumenting the synthesis stage of development\n\n\n\n\n\n\nJun 14, 2023\n\n\nMatt Triano\n\n\n\n\n\n\n  \n\n\n\n\nWorking with Notebooks\n\n\n\n\n\nHow to develop a post in a notebook\n\n\n\n\n\n\nJun 12, 2023\n\n\nMatt Triano\n\n\n\n\n\n\n  \n\n\n\n\nSetting up Conda\n\n\n\n\n\n\n\nconda\n\n\nsetup\n\n\ntutorial\n\n\n\n\n\n\n\n\n\n\n\nJul 22, 2022\n\n\nMatt Triano\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/009_reproducing_a_conda_env/Manually_reproducing_an_old_env.html",
    "href": "posts/009_reproducing_a_conda_env/Manually_reproducing_an_old_env.html",
    "title": "How to Manually Reproduce a Conda Env",
    "section": "",
    "text": "This post will demonstrate how to reproduce an old conda env (that wasn’t exported to an environment.yml file at the time of analysis/usage) needed to rerun old analysis1."
  },
  {
    "objectID": "posts/009_reproducing_a_conda_env/Manually_reproducing_an_old_env.html#step-0.-configure-conda",
    "href": "posts/009_reproducing_a_conda_env/Manually_reproducing_an_old_env.html#step-0.-configure-conda",
    "title": "How to Manually Reproduce a Conda Env",
    "section": "2.1 Step 0. Configure conda",
    "text": "2.1 Step 0. Configure conda\nInstall conda and configure it as shown in steps 3 & 4 here."
  },
  {
    "objectID": "posts/009_reproducing_a_conda_env/Manually_reproducing_an_old_env.html#step-1.-determine-packages-used-in-the-old-work",
    "href": "posts/009_reproducing_a_conda_env/Manually_reproducing_an_old_env.html#step-1.-determine-packages-used-in-the-old-work",
    "title": "How to Manually Reproduce a Conda Env",
    "section": "2.2 Step 1. Determine packages used in the old work",
    "text": "2.2 Step 1. Determine packages used in the old work\nLook at the old analysis and any available metadata to determine:\n\nWhen was the analysis run?\nWhat packages were used?\n\nFor this demonstration, I’m reproducing an env I used to analyze crime and prison data back in 2018. Specifically, I want to produce an env that enables me to rerun these notebooks:\n\nCrime and Prisons part 1\nCrime and Prisons part 2\nCrime and Prisons part 3\n\n\n2.2.1 Determining when the analysis was run\nLooking at the latest commits for these notebooks, we can set an upper bound on versions used. The latest commits for these notebooks are:\n\nPart 1: June 26, 2018\nPart 2: Aug 16, 2018\nPart 3: June 15, 2018\n\nFrom the sidequest described in Section 4.1.1, I’ve decided on using June 15, 2018 as the upper-bound date for analysis.\n\n\n2.2.2 Determining used packages\nFor this, I simply look at the import statements, which are compiled below.\nimport pandas as pd\nimport numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport matplotlib.ticker as ticker\nfrom IPython.core.display import display, HTML\nimport os\nfrom bokeh.sampledata.us_states import data as states\nfrom bokeh.plotting import figure, show, output_notebook\nfrom bokeh.models import HoverTool, ColumnDataSource\nfrom bokeh.models import LinearColorMapper, ColorBar, BasicTicker\nThis boils down to [pandas, numpy, seaborn, matplotlib, IPython, and bokeh] (there’s also os, but that’s a python built-in)."
  },
  {
    "objectID": "posts/009_reproducing_a_conda_env/Manually_reproducing_an_old_env.html#step-2.-determine-max-versions-at-analysis-time",
    "href": "posts/009_reproducing_a_conda_env/Manually_reproducing_an_old_env.html#step-2.-determine-max-versions-at-analysis-time",
    "title": "How to Manually Reproduce a Conda Env",
    "section": "2.3 Step 2. Determine max versions at analysis-time",
    "text": "2.3 Step 2. Determine max versions at analysis-time\nFirst, I want to determine the version of python to use. Looking at the release dates of python versions, we see that python v3.6 was released on 2016-12-23 and python v3.7 was released on 2018-06-27, so the it’s most likely that python v3.6 was used.\n\n\n\n\n\n\nCorroboration\n\n\n\n\n\nLooking at the raw file, specifically a few lines from the very bottom of the document, the metadata block indicates the kernel used python v3.6.4\n\n\n\nNext, I want to determine max versions for pandas, numpy, seaborn, matplotlib, IPython, and bokeh. I know pandas uses numpy and seaborn uses matplotlib, so I can ignore numpy and matplotlib.\nI’ll look at each package’s releases page to see the last version before the cutoff date.\n\npandas: v0.24.2\nseaborn: v0.8.1\nbokeh: v0.12.16\n\n\n\n\n\n\n\nCorroboration\n\n\n\n\n\nLooking at the raw file, specifically by ctrl+f searching “version”, we see that bokeh v0.12.16 was used.\n\n\n\nAlso, IPython was included as it’s a dependency of the (jupyter) notebook package (which I used to develop the notebooks). There will probably be several other infrastructural\n\njupyter notebook: v5.5.0"
  },
  {
    "objectID": "posts/009_reproducing_a_conda_env/Manually_reproducing_an_old_env.html#step-3.-create-the-env-and-register-it-as-a-notebook-kernel",
    "href": "posts/009_reproducing_a_conda_env/Manually_reproducing_an_old_env.html#step-3.-create-the-env-and-register-it-as-a-notebook-kernel",
    "title": "How to Manually Reproduce a Conda Env",
    "section": "2.4 Step 3. Create the env and register it as a notebook kernel",
    "text": "2.4 Step 3. Create the env and register it as a notebook kernel\nFrom the prior step, we determined the following version constraints.\n\npython=3.6.4\npandas&lt;=0.24.2\nseaborn&lt;=0.8.1\nbokeh==0.12.16\nnotebook&lt;=5.5.0\n\nI’ll run the command below to create a conda env named prisons_post_env that meets those constraints.\nconda create --name prisons_post_env \"python=3.6.4\" \"pandas&lt;=0.24.2\" \"seaborn&lt;=0.8.1\" \"bokeh=0.12.16\" \"notebook&lt;=5.5.0\"\nActivate that conda env\nconda activate prisons_post_env\nand register that conda env as a notebook kernel\n(prisons_post_env) ...$ python -m ipykernel install --user --name prisons_post_env --display-name \"(prisons_post_env)\""
  },
  {
    "objectID": "posts/009_reproducing_a_conda_env/Manually_reproducing_an_old_env.html#step-4.-attempt-to-reproduce-prior-results-and-troubleshoot-issues",
    "href": "posts/009_reproducing_a_conda_env/Manually_reproducing_an_old_env.html#step-4.-attempt-to-reproduce-prior-results-and-troubleshoot-issues",
    "title": "How to Manually Reproduce a Conda Env",
    "section": "2.5 Step 4. Attempt to reproduce prior results and troubleshoot issues",
    "text": "2.5 Step 4. Attempt to reproduce prior results and troubleshoot issues\nNow you can start up a notebook server (I’ve specified a port number as I’m already running a jupyterlab server on the default port, 8888)\njupyter notebook --port=9494\n\n2.5.1 Troubleshooting 1\nWhile trying to open up the part 2 notebook, the connection attempt hung and the terminal showed an error.\n...\n~/miniconda3/envs/prisons_post_env/lib/python3.6/site-packages/notebook/base/zmqhandlers.py:284:  RuntimeWarning: coroutine 'WebSocketHandler.get' was never awaited\nGoogling the error took me to a Stack Overflow question that indicates package tornado v6+ caused the issue, so let’s downgrade tornado in our env\n(prisons_post_env) ...$ conda install -c conda-forge \"tornado&lt;6\" --freeze-installed\nthen restart our notebook server (press ctrl+c in the terminal, shut it down, then start it back up with the earlier jupyter notebook command). When you reopen the Crime_and_Prisons_part2.ipynb notebook, you should find that it successfully connects to the kernel and you can run through cells. At least up until cell that calls the plot_male_v_female_by_state_sea() function.\n\n\n2.5.2 Troubleshooting 2\nUpon attempting to run that cell, you will see another error message.\n~/miniconda3/envs/prisons_post_env/lib/python3.6/site-packages/matplotlib/artist.py in update(self, props)\n...\nAttributeError: 'Rectangle' object has no property 'normed'\nAfter a few minutes of googling the error message along with the word matplotlib, I’ve determined that the problem is that the installed seaborn version’s distplot() function calls matplotlib’s hist() plotter function using a keyword argument, normed, that was changed in the matplotlib v3.2.0 release. And by running this\nimport matplotlib\nmatplotlib.__version__\nI see this env has matplotlib v3.3.2 installed. So let’s downgrade matplotlib.\n(prisons_post_env) ...$ conda install -c conda-forge \"matplotlib&lt;3.2\"\nLooking at the installation plan, I see that conda wants to upgrade a lot of packages in violation of the earlier constraints. I also tried adding the --freeze-installed option, but conda still wanted to make updates including these.\n  bokeh                                      0.12.16-py36_0 --&gt; 2.3.3-py36h5fab9bb_0\n  notebook                                     5.5.0-py36_0 --&gt; 6.3.0-py36h5fab9bb_0\n  pandas                              0.24.2-py36hb3f55d8_1 --&gt; 1.1.5-py36h284efc9_0\n  python                                            3.6.4-0 --&gt; 3.6.15-hb7a2778_0_cpython\n  seaborn                                        0.8.1-py_1 --&gt; 0.11.2-hd8ed1ab_0\n  tornado                           5.1.1-py36h14c3975_1000 --&gt; 6.1-py36h8f6f2f9_1\n  ...\nSo let’s just completely remove and remake the env with all of our constraints, old and new.\nAfter shutting down the jupyter notebook server and ensuring the env is not activated in any open terminal, remove the env directory\nrm -r ~/miniconda3/envs/prisons_post_env/\nthen recreate the env with our additional constraints. Through a fair bit of trial and error, I determined that one of my preferred configs (namely prioritizing the conda-forge channel) was making it impossible to reconcile these constraints, so I overrode the configured channels in favor of the default channel that I was probably using 5 years ago. I’ll also add on the xlrd package, as the part3 notebook loads a .xls file.\nconda create --name prisons_post_env --override-channels --channel defaults \"python=3.6.4\" \"pandas&lt;=0.24.2\" \"seaborn&lt;=0.8.1\" \"bokeh=0.12.16\" \"notebook&lt;=5.5.0\" \"tornado&lt;6\" \"matplotlib=2.2.2\" xlrd\nThen activate and re-register the env\nconda activate prisons_post_env\n(prisons_post_env) ...$ python -m ipykernel install --user --name prisons_post_env --display-name \"(prisons_post_env)\"\nand restart the notebook server.\nNow all three of those old notebooks can be run successfully (after collecting and locating the data in the right places).\n\n\n\n\n\n\nWhy did changing the conda channel make the dependencies solvable?\n\n\n\n\n\nYou may wonder “How could changing the package source (aka ‘channel’) make the env solvable? The package versions were the same!”\nThat’s a good observation and intuition! If converting a python package into a conda package was impossible to mess up, there wouldn’t be any difference in conda packages for a given python package version across channels. But conda isn’t just a tool for packaging python code; it’s a tool for packaging and distributing any executable, and that often means instructions for building the package and for resolving dependencies are needed. In essence, you need a recipe for making the package. In conda terms, that recipe is a package’s meta.yaml file, and the file provides places to point to build scripts and define dependencies. Each conda channel is maintained separately, so each can have different meta.yaml file for a given python package version. Consequently, if dependencies are inconsistent across channels, an env that’s consistent when pulling exclusively from one channel may be unresolvable when pulling exclusively from another channel."
  },
  {
    "objectID": "posts/009_reproducing_a_conda_env/Manually_reproducing_an_old_env.html#step-5.-export-the-env",
    "href": "posts/009_reproducing_a_conda_env/Manually_reproducing_an_old_env.html#step-5.-export-the-env",
    "title": "How to Manually Reproduce a Conda Env",
    "section": "2.6 Step 5. Export the env",
    "text": "2.6 Step 5. Export the env\nNow that we have a working env, let’s export both the full specification and a cross-platform specification (which only includes the explicitly requested packages).\n\n\nCode\n!conda env export -n prisons_post_env &gt; environment.yml\n\n\n\n\nCode\n!conda env export -n prisons_post_env --from-history &gt; environment_cross_platform.yml"
  },
  {
    "objectID": "posts/009_reproducing_a_conda_env/Manually_reproducing_an_old_env.html#side-projects",
    "href": "posts/009_reproducing_a_conda_env/Manually_reproducing_an_old_env.html#side-projects",
    "title": "How to Manually Reproduce a Conda Env",
    "section": "4.1 Side Projects",
    "text": "4.1 Side Projects\nWhile working through technical projects, little problems tangential to the main task often pop out and block progress. Often these side quests can be ignored, but\n\n4.1.1 git diff side project\nI don’t recall why I updated Parts 1 and 2 after Part 3. I doubt I made substantive changes, but as I’m using metadata of git commits to determine changes, it only makes sense to look at the diffs. Unfortunately, while github indicates a relatively small number of lines were modified, the diffs are too large to display in browser and I have to review in a locally to see the diffs.\n\n\n\n\n\n\nNote\n\n\n\n\n\nThis is a well-known drawback of jupyter notebooks; plots get represented by very long plaintext strings and rerunning a notebook often changes every line in version control, so diffs can be hard to review).\n\n\n\nSo I cloned the repo, copied down the hash of the commit I’m interested in (commit 0857e6c), and looked at the diffs of that file in that commit via\ngit diff 0857e6c^..0857e6c -- Crime_and_Prisons_part2.ipynb\nMost of the changes only changed the cell execution-order number or uuid-looking tags. There may also have been changes to the extremely long string representations used to render plots, but they were too long to crosscheck. In fact, those long strings took so long to page through that I stopped reviewing that way and just compared the rendered notebooks (pre-commit vs commit) and concluded there weren’t any substantive changes, so the timestamp from the earlier commit is adequate."
  },
  {
    "objectID": "posts/009_reproducing_a_conda_env/Manually_reproducing_an_old_env.html#footnotes",
    "href": "posts/009_reproducing_a_conda_env/Manually_reproducing_an_old_env.html#footnotes",
    "title": "How to Manually Reproduce a Conda Env",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nContext: Over the years, I’ve writen up a number of posts for a number of different personal blogs, and I want to consolidate those posts into one platform. Many of my posts involved leveraging the capabilities of jupyter notebooks, and while I’ve always used conda envs to avoid polluting my base python environment, I didn’t reliably export my envs or keep separate envs for each project or purpose. So I occassionally run into a situation where I want to rerun old code on a new machine, but I have to go through extra steps to recreate the env.↩︎"
  },
  {
    "objectID": "posts/002_notebook_test/jupyter_quarto.html",
    "href": "posts/002_notebook_test/jupyter_quarto.html",
    "title": "Working with Notebooks",
    "section": "",
    "text": "Code\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nr = np.arange(0, 4, 0.01)\ntheta = 2 * np.pi * r\nfig, ax = plt.subplots(\n  subplot_kw = {'projection': 'polar'} \n)\nax.plot(theta, r)\nax.set_rticks([0.5, 1, 1.5, 2])\nax.grid(True)\nplt.show()\n\n\n\n\n\nFigure 1: A line plot on a polar axis\n\n\n\n\n\nDevelop a notebook\nRender that notebook via\n(quarto_env) user@hostname:~/...$ quarto render notebook_name.ipynb\nPreview your document via\n(quarto_env) user@hostname:~/...$ quarto preview quarto_blog/\nNote: Make sure draft: false in your document, or it won’t render.\n\n\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/006_great_expectations_setup/data_monitoring_w_gx_1_setup.html",
    "href": "posts/006_great_expectations_setup/data_monitoring_w_gx_1_setup.html",
    "title": "Data Quality Monitoring with Great Expectations",
    "section": "",
    "text": "Imports and path-definition\nfrom collections import Counter\nimport datetime as dt\nfrom pathlib import Path\nfrom urllib.request import urlretrieve\n\nimport geopandas as gpd\nimport pandas as pd\n\nPROJECT_DIR = Path(\".\").resolve()\nPROJECT_DATA_DIR = PROJECT_DIR.joinpath(\"data\")\nGreat Expectations (or GX for short) is an open-source Python-based library that brings the idea of “testing” to your data. It enables you to define expectations for properties of your datasets (like records per batch, distribution of values in a column, columns in a table, etc) and check that the data meets those expectations when the data is updated."
  },
  {
    "objectID": "posts/006_great_expectations_setup/data_monitoring_w_gx_1_setup.html#step-0-great-expectations-setup",
    "href": "posts/006_great_expectations_setup/data_monitoring_w_gx_1_setup.html#step-0-great-expectations-setup",
    "title": "Data Quality Monitoring with Great Expectations",
    "section": "Step 0: Great Expectations Setup",
    "text": "Step 0: Great Expectations Setup\nFirst, you’ll need to install the great_expectations. If you already have conda installed on your machine, you can easily set up a conda env just like the one used to run this notebook by: 1. copying the gx_env_environment.yml file in the same dir as this notebook file to your machine, 2. open a terminal and navigate to the dir with that new file, and 3. run command conda env create -f environment.yml\nIf you don’t have conda but would like to, check out my opinionated conda install and configuration post.\n\n\nCollecting and preprocessing sample data for this post\nPROJECT_DATA_DIR.mkdir(exist_ok=True)\n\n# First, we need to download the data to our local machine.\nurl = \"https://data.cityofchicago.org/api/geospatial/4ijn-s7e5?method=export&format=GeoJSON\"\nfull_file_path = PROJECT_DATA_DIR.joinpath(\"full_food_inspections.geojson\")\nif not full_file_path.is_file():\n    urlretrieve(url=url, filename=full_file_path)\nfood_inspection_gdf = gpd.read_file(full_file_path)\n\n# For some reason, Socrata adds on these four always-null location columns on\n#   to geospatial exports. I'm going to remove them.\nlocation_cols = [\"location_state\", \"location_zip\", \"location_address\", \"location_city\"]\n# uncomment the lines below to confirm those columns are always empty\n# print(\"Rows with a non-null value in these location_xxx columns:\")\n# display(food_inspection_gdf[location_cols].notnull().sum())\nfood_inspection_gdf = food_inspection_gdf.drop(columns=location_cols)\n\n# That column ordering is a bit chaotic, so I'll reorder them (for readability).\ncol_order = [\n    \"inspection_id\", \"inspection_date\", \"dba_name\", \"aka_name\", \"license_\", \"facility_type\",\n    \"risk\", \"inspection_type\", \"results\", \"address\", \"city\", \"state\", \"zip\", \"violations\",\n    \"longitude\", \"latitude\", \"geometry\"\n]\nfood_inspection_gdf = food_inspection_gdf[col_order].copy()\n\n# I also want to break this into batches based on the dates, so I need to cast\n#   the `inspection_date` to a datetime type.\nfood_inspection_gdf[\"inspection_date\"] = pd.to_datetime(\n    food_inspection_gdf[\"inspection_date\"]\n)\n\n# I'll also cast string and numeric features to their proper dtypes.\n# food_inspection_gdf = food_inspection_gdf.convert_dtypes()\nfood_inspection_gdf[\"inspection_id\"] = food_inspection_gdf[\"inspection_id\"].astype(\"Int64\")\nfood_inspection_gdf[\"longitude\"] = food_inspection_gdf[\"longitude\"].astype(float)\nfood_inspection_gdf[\"latitude\"] = food_inspection_gdf[\"latitude\"].astype(float)\n\n# I'll also just make all string uppercase (to reduce cardinality)\nstr_cols = list(food_inspection_gdf.head(2).select_dtypes(include=\"object\").columns)\nfood_inspection_gdf[str_cols] = food_inspection_gdf[str_cols].apply(lambda x: x.str.upper())\n\n\nIn the (folded up) cell below, we split the dataset into batches and write each batch to file in this post’s ./data directory.\n\n\nAnd here we split the dataset into batches and write each batch to file in this post’s ./data directory.\n# I want to split the data into 1-month batches, so I need to get the first day of the month\n#   for every month between the earliest inspection and the month after the latest inspection\n#   in our food inspection dataset.\nmonth_start_dates = pd.date_range(\n    start=food_inspection_gdf[\"inspection_date\"].min() + pd.DateOffset(months=-1),\n    end=food_inspection_gdf[\"inspection_date\"].max(),\n    freq=\"MS\",\n)\n\n# Here, we'll iterate through each of those month_start_dates, extract the batch of data,\n#   format a filename containing the month_start_date, and write the batch to file.\nfor month_start_date in month_start_dates:\n    batch_period = pd.to_datetime(month_start_date).strftime(\"%Y_%m\")\n    batch_data = food_inspection_gdf.loc[\n        food_inspection_gdf[\"inspection_date\"].between(\n            left=month_start_date,\n            right=month_start_date + pd.DateOffset(months=1),\n            inclusive=\"left\")\n    ].copy()\n    batch_file_path = PROJECT_DATA_DIR.joinpath(f\"food_inspection_batch_{batch_period}.parquet\")\n    if not batch_file_path.is_file():\n        batch_data.to_parquet(batch_file_path, index=False)"
  },
  {
    "objectID": "posts/006_great_expectations_setup/data_monitoring_w_gx_1_setup.html#step-1-create-or-load-great-expectations-datacontext",
    "href": "posts/006_great_expectations_setup/data_monitoring_w_gx_1_setup.html#step-1-create-or-load-great-expectations-datacontext",
    "title": "Data Quality Monitoring with Great Expectations",
    "section": "Step 1: Create or Load Great Expectations DataContext",
    "text": "Step 1: Create or Load Great Expectations DataContext\nA DataContext is your primary tool for configuring your project and accessing project resources or GX methods. When you first create a DataContext for your project, GX will create a directory named /great_expectations in the project_root_dir directory.\nThe code below will create a new DataContext if one doesn’t already exist in the PROJECT_DIR directory, and then load a DataContext instance from that PROJECT_DIR. Great Expectations defaults to collecting anonymized usage statistics, but you can disable that for your context by setting usage_statistics_enabled=False.\n\nimport great_expectations as gx\nfrom great_expectations.data_context import FileDataContext\n\ncontext = FileDataContext.create(project_root_dir=PROJECT_DIR, usage_statistics_enabled=False)\n\nThis tutorial uses a local FileDataContext, but GX also supports CloudDataContexts and EphemeralDataContexts.\n\n\nKinds of DataContexts\n[el for el in dir(gx.data_context) if el.endswith(\"Context\")]\n\n\n['AbstractDataContext',\n 'BaseDataContext',\n 'CloudDataContext',\n 'DataContext',\n 'EphemeralDataContext',\n 'FileDataContext']"
  },
  {
    "objectID": "posts/006_great_expectations_setup/data_monitoring_w_gx_1_setup.html#step-2-create-or-load-a-datasource",
    "href": "posts/006_great_expectations_setup/data_monitoring_w_gx_1_setup.html#step-2-create-or-load-a-datasource",
    "title": "Data Quality Monitoring with Great Expectations",
    "section": "Step 2: Create or load a Datasource",
    "text": "Step 2: Create or load a Datasource\nA GX Datasource connects you to a source of data and gives you methods to define and access DataAssets.\nThe code below will check the DataContext context for a Datasource with the given datasource_name, and either load or create a local filesystem Datasource instance.\n\ndatasource_name = \"food_inspection_datasource\"\n\nif any(el[\"name\"] == datasource_name for el in context.list_datasources()):\n    print(f\"Datasource with name '{datasource_name}' found; loading now\")\n    datasource = context.get_datasource(datasource_name)\nelse:\n    print(f\"No Datasource with name '{datasource_name}' found; creating now\")\n    datasource = context.sources.add_pandas_filesystem(\n        name=datasource_name,\n        base_directory=PROJECT_DATA_DIR\n    )\n\nNo Datasource with name 'food_inspection_datasource' found; creating now\n\n\n\n\nOther kinds of GX Datasources\n[el for el in dir(context.sources) if el.startswith(\"add_\") and \"_update_\" not in el]\n\n\n['add_pandas',\n 'add_pandas_abs',\n 'add_pandas_dbfs',\n 'add_pandas_filesystem',\n 'add_pandas_gcs',\n 'add_pandas_s3',\n 'add_postgres',\n 'add_spark',\n 'add_spark_abs',\n 'add_spark_dbfs',\n 'add_spark_filesystem',\n 'add_spark_gcs',\n 'add_spark_s3',\n 'add_sql',\n 'add_sqlite']"
  },
  {
    "objectID": "posts/006_great_expectations_setup/data_monitoring_w_gx_1_setup.html#step-3-define-dataassets-in-that-datasource",
    "href": "posts/006_great_expectations_setup/data_monitoring_w_gx_1_setup.html#step-3-define-dataassets-in-that-datasource",
    "title": "Data Quality Monitoring with Great Expectations",
    "section": "Step 3: Define DataAssets in that Datasource",
    "text": "Step 3: Define DataAssets in that Datasource\nA GX DataAsset specifies a collection of records in a Datasource and the method for accessing those records.\nThe code below checks if a DataAsset with the given name exists in the datasource, loading it if it exists, or specifying it if not. In the part that specifies the DataAsset, note that we set the name of the asset, specify that the data is in parquet files, and provide a regex pattern for the file_names and also defines variable-names for the year and month parts each file_name. We can use those year and month variables to specify how DataAssets should be split into batches and the order of those batches.\n\ndata_asset_name = \"food_inspections_asset\"\n\nif data_asset_name not in datasource.get_asset_names():\n    print(f\"Creating data asset {data_asset_name}\")\n    data_asset = datasource.add_parquet_asset(\n        name=data_asset_name,\n        batching_regex = r\"food_inspection_batch_(?P&lt;year&gt;\\d{4})_(?P&lt;month&gt;\\d{2})\\.parquet\"\n    )\nelse:\n    data_asset = datasource.get_asset(data_asset_name)\ndata_asset = data_asset.add_sorters([\"+year\", \"+month\"])\n\nCreating data asset food_inspections_asset\n\n\n\n\nOther data file formats GX supports\n[el for el in dir(datasource) if el.startswith(\"add_\")]\n\n\n['add_csv_asset',\n 'add_excel_asset',\n 'add_feather_asset',\n 'add_fwf_asset',\n 'add_hdf_asset',\n 'add_html_asset',\n 'add_json_asset',\n 'add_orc_asset',\n 'add_parquet_asset',\n 'add_pickle_asset',\n 'add_sas_asset',\n 'add_spss_asset',\n 'add_stata_asset',\n 'add_xml_asset']"
  },
  {
    "objectID": "posts/006_great_expectations_setup/data_monitoring_w_gx_1_setup.html#step-4-create-expectations-for-a-dataasset",
    "href": "posts/006_great_expectations_setup/data_monitoring_w_gx_1_setup.html#step-4-create-expectations-for-a-dataasset",
    "title": "Data Quality Monitoring with Great Expectations",
    "section": "Step 4: Create Expectations for a DataAsset",
    "text": "Step 4: Create Expectations for a DataAsset\nA GX Expectation is a verifiable assertion about some property of a DataAsset, and defining Expectations both enables GX to check that data meets expectations and enables domain experts to explicitly represent and communicate Expectations for data.\nGX supports hundreds of different Expectations and catalogs them in the Expectation Gallery (although not all Expectations are implemented for all kinds of Datasources). GX also provides tools to aid in several workflows for defining suites of Expectations, including the GX Data Assistant workflow (used below), which builds a suite of Expectations by profiling batches of data.\nIn the code below, we create a new Expectation suite (on lines 3-5), organize batches of data (on lines 6-7), and use the data assistant to profile the DataAsset based on our batches of data (on lines 8-11).\n\nexpectation_suite_name = \"food_inspections_suite\"\n\nexpectation_suite = context.add_or_update_expectation_suite(\n    expectation_suite_name=expectation_suite_name\n)\nbatch_request = data_asset.build_batch_request()\nbatches = data_asset.get_batch_list_from_batch_request(batch_request)\ndata_assistant_result = context.assistants.onboarding.run(\n    batch_request=batch_request,\n    exclude_column_names=[\"inspection_date\", \"geometry\"],\n)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThe profiler will sequentially generate a lot of progress bars (like these) as it profiles dataset features.\n\n\n\nProfiler output\n\n\n\ndata_assistant_result.plot_expectations_and_metrics()\n\n64 Expectations produced, 23 Expectation and Metric plots implemented\nUse DataAssistantResult.show_expectations_by_domain_type() or\nDataAssistantResult.show_expectations_by_expectation_type() to show all produced Expectations\n\n\n\n                \n                \n\n\n\n                \n            \n\n\n\n                \n            \n\n\n\n\n\n\n\n\n\nData Assistant Plot Inspector plots\nAfter the Data Assistant finishes profiling, it outputs results to a variable we named data_assistant_result, and you can explore the results across batches by calling data_assistant_result.plot_expectations_and_metrics() and selecting the expectation and column you’re interested in.\n   \n\n\nExtracting, [optionally] Editing, and Committing our Expectation Suite to our DataContext\nIf we’re content with the Expectations generated by the Data Assistant’s profiler, we can simply extract the Expectations and add them to our context via\nexpectation_suite = data_assistant_result.get_expectation_suite(\n    expectation_suite_name=expectation_suite_name\n)\nsaved_suite = context.add_or_update_expectation_suite(expectation_suite=expectation_suite)\nIn a future post I’ll go into further depth on methods for editing Expectations, but here I’ll show how to inspect and remove Expectations.\n\nexpectation_suite = data_assistant_result.get_expectation_suite(\n    expectation_suite_name=expectation_suite_name\n)\n\n\n\nCounts of Expectations by Type\nprint(f\"Counts of Expectations by Expectation-type:\")\nexpecs_by_type = expectation_suite.get_grouped_and_ordered_expectations_by_expectation_type()\ndisplay(Counter([ex._expectation_type for ex in expecs_by_type]))\n\n\nCounts of Expectations by Expectation-type:\n\n\nCounter({'expect_column_value_lengths_to_be_between': 12,\n         'expect_column_values_to_match_regex': 11,\n         'expect_column_proportion_of_unique_values_to_be_between': 7,\n         'expect_column_unique_value_count_to_be_between': 7,\n         'expect_column_values_to_be_in_set': 7,\n         'expect_column_values_to_not_be_null': 4,\n         'expect_column_max_to_be_between': 2,\n         'expect_column_mean_to_be_between': 2,\n         'expect_column_median_to_be_between': 2,\n         'expect_column_min_to_be_between': 2,\n         'expect_column_quantile_values_to_be_between': 2,\n         'expect_column_stdev_to_be_between': 2,\n         'expect_column_values_to_be_between': 2,\n         'expect_table_columns_to_match_set': 1,\n         'expect_table_row_count_to_be_between': 1})\n\n\n\n\nCounts of Expectations by Column\nprint(f\"Counts of Expectations by Column-name:\")\nexpecs_by_col = expectation_suite.get_grouped_and_ordered_expectations_by_column()\nexpec_count_by_col = {col: len(col_expecs) for col, col_expecs in expecs_by_col[0].items()}\ndisplay(sorted(expec_count_by_col.items(), key=lambda x: x[1], reverse=True))\n\n\nCounts of Expectations by Column-name:\n\n\n[('longitude', 7),\n ('latitude', 7),\n ('inspection_type', 6),\n ('results', 6),\n ('facility_type', 5),\n ('risk', 5),\n ('city', 5),\n ('state', 5),\n ('zip', 5),\n ('dba_name', 3),\n ('_nocolumn', 2),\n ('address', 2),\n ('aka_name', 2),\n ('license_', 2),\n ('violations', 2)]\n\n\n\n\nExpectation-related methods on our expectation_suite\n[el for el in dir(expectation_suite) if \"_expectation\" in el]\n\n\n['_add_expectation',\n '_get_expectations_by_domain_using_accessor_method',\n '_validate_expectation_configuration_before_adding',\n 'add_expectation',\n 'add_expectation_configurations',\n 'append_expectation',\n 'find_expectation_indexes',\n 'find_expectations',\n 'get_column_expectations',\n 'get_column_pair_expectations',\n 'get_grouped_and_ordered_expectations_by_column',\n 'get_grouped_and_ordered_expectations_by_domain_type',\n 'get_grouped_and_ordered_expectations_by_expectation_type',\n 'get_multicolumn_expectations',\n 'get_table_expectations',\n 'patch_expectation',\n 'remove_all_expectations_of_type',\n 'remove_expectation',\n 'replace_expectation',\n 'show_expectations_by_domain_type',\n 'show_expectations_by_expectation_type']\n\n\n\n\nInspecting Expectation types for a given column\ncol_name = \"longitude\"\n[ex[\"expectation_type\"] for ex in expectation_suite.get_column_expectations() if ex[\"kwargs\"][\"column\"] == col_name]\n\n\n['expect_column_min_to_be_between',\n 'expect_column_max_to_be_between',\n 'expect_column_values_to_be_between',\n 'expect_column_quantile_values_to_be_between',\n 'expect_column_median_to_be_between',\n 'expect_column_mean_to_be_between',\n 'expect_column_stdev_to_be_between']\n\n\nSome Expectations are redundant, such as expect_column_min_to_be_between and expect_column_max_to_be_between. I’ll remove them.\n\ncol_name = \"longitude\"\nexpectation_types_to_remove = [\n    \"expect_column_min_to_be_between\", \"expect_column_max_to_be_between\"\n]\ncol_expectations_w_type = [\n    ex for ex in expectation_suite.get_column_expectations()\n    if (ex[\"kwargs\"][\"column\"] == col_name) and (ex[\"expectation_type\"] in expectation_types_to_remove)\n]\ncol_expectations_w_type\n\n[{\"kwargs\": {\"column\": \"longitude\", \"strict_min\": false, \"min_value\": -87.91442843927047, \"strict_max\": false, \"max_value\": -87.81649552747085}, \"meta\": {\"profiler_details\": {\"metric_configuration\": {\"metric_name\": \"column.min\", \"domain_kwargs\": {\"column\": \"longitude\"}, \"metric_value_kwargs\": null}, \"num_batches\": 162}}, \"expectation_type\": \"expect_column_min_to_be_between\"},\n {\"kwargs\": {\"column\": \"longitude\", \"strict_min\": false, \"min_value\": -87.5510612280602, \"strict_max\": false, \"max_value\": -87.5250941359867}, \"meta\": {\"profiler_details\": {\"metric_configuration\": {\"metric_name\": \"column.max\", \"domain_kwargs\": {\"column\": \"longitude\"}, \"metric_value_kwargs\": null}, \"num_batches\": 162}}, \"expectation_type\": \"expect_column_max_to_be_between\"}]\n\n\n\nprint(f\"Expectations prior to removal: {len(expectation_suite.expectations)}\")\nfor expectation_to_remove in col_expectations_w_type:\n    removed_expec = expectation_suite.remove_expectation(expectation_to_remove)\nprint(f\"Expectations after removal:    {len(expectation_suite.expectations)}\")\n\nExpectations prior to removal: 64\nExpectations after removal:    62\n\n\nWe can also get rid of every instance of an Expectation type.\n\nprint(f\"Removing Expectation types:\")\nfor expec_type in expectation_types_to_remove:\n    print(f\"  - {expec_type}\")\nprint(f\"Expectations prior to removal: {len(expectation_suite.expectations)}\")\nremoved_expecs = expectation_suite.remove_all_expectations_of_type(expectation_types=expectation_types_to_remove)\nprint(f\"Expectations after removal:    {len(expectation_suite.expectations)}\")\n\nRemoving Expectation types:\n  - expect_column_min_to_be_between\n  - expect_column_max_to_be_between\nExpectations prior to removal: 62\nExpectations after removal:    60\n\n\nAfter reviewing and editing Expectations, the Expectation Suite must be committed to the DataContext.\n\nsaved_suite = context.add_or_update_expectation_suite(expectation_suite=expectation_suite)"
  },
  {
    "objectID": "posts/006_great_expectations_setup/data_monitoring_w_gx_1_setup.html#step-5-setup-a-checkpoint-to-check-expectations",
    "href": "posts/006_great_expectations_setup/data_monitoring_w_gx_1_setup.html#step-5-setup-a-checkpoint-to-check-expectations",
    "title": "Data Quality Monitoring with Great Expectations",
    "section": "Step 5: Setup a Checkpoint to check Expectations",
    "text": "Step 5: Setup a Checkpoint to check Expectations\nA GX Checkpoint configures the validation process for an Expectation Suite.\nSpecifically, a Checkpoint defines: * the Expectation Suite to evaluate, * the data Batches to evaluate against, and * the actions to take after evaluation.\nWe’ll take the actions of compiling a report of results and committing our Checkpoint to our DataContext, but you can also configure a Checkpoint to send results via a email or Slack notification.\n\ncheckpoint_name = \"food_inspections_checkpoint\"\n\ncheckpoint = gx.checkpoint.SimpleCheckpoint(\n    name=checkpoint_name,\n    data_context=context,\n    validations=[\n        {\n            \"batch_request\": batch_request,\n            \"expectation_suite_name\": expectation_suite_name,\n        },\n    ],\n)\ncheckpoint_result = checkpoint.run()\n\n\n\n\n\ncontext.build_data_docs()\n\n{'local_site': 'file:///home/matt/projects/blogs/quarto_blog/posts/006_great_expectations_setup/great_expectations/uncommitted/data_docs/local_site/index.html'}\n\n\nTo view the generated validation report (or Data Docs), open the file .../great_expectations/uncommitted/data_docs/local_site/index.html and select the validation run you want to review. You may have to click Trust HTML (upper left corner in Jupyterlab) to navigate the document.\n \n\ncontext.add_checkpoint(checkpoint=checkpoint)\n\n{\n  \"action_list\": [\n    {\n      \"name\": \"store_validation_result\",\n      \"action\": {\n        \"class_name\": \"StoreValidationResultAction\"\n      }\n    },\n    {\n      \"name\": \"store_evaluation_params\",\n      \"action\": {\n        \"class_name\": \"StoreEvaluationParametersAction\"\n      }\n    },\n    {\n      \"name\": \"update_data_docs\",\n      \"action\": {\n        \"class_name\": \"UpdateDataDocsAction\"\n      }\n    }\n  ],\n  \"batch_request\": {},\n  \"class_name\": \"SimpleCheckpoint\",\n  \"config_version\": 1.0,\n  \"evaluation_parameters\": {},\n  \"module_name\": \"great_expectations.checkpoint\",\n  \"name\": \"food_inspections_checkpoint\",\n  \"profilers\": [],\n  \"runtime_configuration\": {},\n  \"validations\": [\n    {\n      \"batch_request\": {\n        \"datasource_name\": \"food_inspection_datasource\",\n        \"data_asset_name\": \"food_inspections_asset\",\n        \"options\": {}\n      },\n      \"expectation_suite_name\": \"food_inspections_suite\"\n    }\n  ]\n}\n\n\nYou can easily integrate a defined Checkpoint into a pipeline with just a few lines of code (and another dependency).\n\nimport great_expectations as gx\n\ncontext = gx.get_context(context_root_dir=PROJECT_DIR.joinpath(\"great_expectations\"))\nretrieved_checkpoint = context.get_checkpoint(name=\"food_inspections_checkpoint\")\nretrieved_checkpoint_result = retrieved_checkpoint.run()\nif not retrieved_checkpoint_result[\"success\"]:\n    print(f\"Failed Validation Checkpoint!\")\n    # or raise Exception(\"if you'd rather handle validation failures that way\")"
  },
  {
    "objectID": "posts/006_great_expectations_setup/data_monitoring_w_gx_1_setup.html#next-steps",
    "href": "posts/006_great_expectations_setup/data_monitoring_w_gx_1_setup.html#next-steps",
    "title": "Data Quality Monitoring with Great Expectations",
    "section": "Next Steps",
    "text": "Next Steps\nIn future posts, I’ll dive deeper into the Expectation-setting process, demonstrate a workflow with a PostgreSQL Datasource and DataAssets (where GX really shines), and explore strategies for integrating data monitoring into production ETL/ELT pipelines (like those in my personal data warehousing platform)."
  },
  {
    "objectID": "posts/007_midjourney_experiments/midjourney_experiments.html",
    "href": "posts/007_midjourney_experiments/midjourney_experiments.html",
    "title": "Midjourney Experiments",
    "section": "",
    "text": "Over the past decade, the field of generative AI has made amazing progress. Driven by breakthrough advances in machine learning modeling strategies (primarily the GAN 1 in 2014 and the Transformer 2 in 2017) coupled with exponential growth of the amount of available computing power 3, generative AI applications for different data formats started appearing in the late 2010s and early 2020s. Notable examples include\nOver the past two years, text-to-image platforms have jockeyed to lead that market with DALL-E generating a massive amount of initial hype, but Midjourney has emerged as the clearly superior service, despite having the least conventional workflow.\nMidjourney’s model was the first to reliably produce human forms (including well-formed hands, a difficulty for prior models), its best outputs are unambiguously superior to the best outputs from any other model, and it’s tremendously fun to play with. Midjourney isn’t free to use (the least expensive tier is \\$10/mo), but it’s free to look at (or search) the endless stream of images generated by other Midjourney users, it’s easy to unsubscribe, and it’s hard to not get at least \\$10 worth of fun out of the service.\nIn this post, I’ll point out the important parts of the interface, walk through setup and (un)subscription, demonstrate useful commands, and show examples of what Midjourney can do."
  },
  {
    "objectID": "posts/007_midjourney_experiments/midjourney_experiments.html#midjourney-interface",
    "href": "posts/007_midjourney_experiments/midjourney_experiments.html#midjourney-interface",
    "title": "Midjourney Experiments",
    "section": "Midjourney Interface",
    "text": "Midjourney Interface\nThe Discord interface is packed with buttons, lists, and inputs, but you only need to know 5 areas you need to know.\n\nThe Midjourney server icon\n\nThis, the ship icon, should always be selected.\n\nThe Channel list\n\nPick any “newbies-##” or “general-##” channel.\n\nThe Prompt input\n\nThis is where you enter prompts or commands for the Midjourney Bot to handle.\n\nThe Search bar\n\nYou can search through all public images in the Midjourney server.\n\nThe message display space\n\nThis is where you’ll find all (public) images generated in the selected channel (either by you or other users).\n\n\n\n\n\nInterface"
  },
  {
    "objectID": "posts/007_midjourney_experiments/midjourney_experiments.html#how-to-unsubscribe",
    "href": "posts/007_midjourney_experiments/midjourney_experiments.html#how-to-unsubscribe",
    "title": "Midjourney Experiments",
    "section": "How to unsubscribe",
    "text": "How to unsubscribe\nTo unsubscribe, go to the Midjourney account page, click Cancel Plan (1, then 2), and then confirm cancellation in the popup. If you’ve used less than 20 GPU minutes (~30 images) in the billing period, you can choose to get a refund."
  },
  {
    "objectID": "posts/007_midjourney_experiments/midjourney_experiments.html#aspect-ratio",
    "href": "posts/007_midjourney_experiments/midjourney_experiments.html#aspect-ratio",
    "title": "Midjourney Experiments",
    "section": "Aspect Ratio",
    "text": "Aspect Ratio\nThe desired aspect ratio for an image.\n\nParameter: --ar l:w or --aspect w:h (where w is width h is height)\nExamples:\n\n--ar 1:1 (square, default)\n--ar 9:16 (good for stories; typical phone screen aspect ratio)\n--ar 19:10 (good for covers or landscapes)\n--ar 4:5 (good for portraits)"
  },
  {
    "objectID": "posts/007_midjourney_experiments/midjourney_experiments.html#quality",
    "href": "posts/007_midjourney_experiments/midjourney_experiments.html#quality",
    "title": "Midjourney Experiments",
    "section": "Quality",
    "text": "Quality\nControls the rendering quality (i.e., amount of CPU time spent generating an image).\n\nParameter: --q value or --quality value\nAccepted values: [0.25, 0.5, 1] (higher values = higher quality, default is 1)\nExample: --q 0.25"
  },
  {
    "objectID": "posts/007_midjourney_experiments/midjourney_experiments.html#stylize",
    "href": "posts/007_midjourney_experiments/midjourney_experiments.html#stylize",
    "title": "Midjourney Experiments",
    "section": "Stylize",
    "text": "Stylize\nInfluences how strongly Midjourney stylizes an image.\n\nParameter: --s value or --stylize value\nAccepted values: 0 to 1000 (lower values = simpler, higher values = more intricate, default is 100)\nExample: --s 1000"
  },
  {
    "objectID": "posts/007_midjourney_experiments/midjourney_experiments.html#negative-prompting",
    "href": "posts/007_midjourney_experiments/midjourney_experiments.html#negative-prompting",
    "title": "Midjourney Experiments",
    "section": "Negative prompting",
    "text": "Negative prompting\nAllows you to strongly signal something should be excluded from an image.\n\nParameter: --no value\nAccepted values: Any text.\nExample: --no plants\n\nThe full list of parameters is available here"
  },
  {
    "objectID": "posts/007_midjourney_experiments/midjourney_experiments.html#papercut",
    "href": "posts/007_midjourney_experiments/midjourney_experiments.html#papercut",
    "title": "Midjourney Experiments",
    "section": "Papercut",
    "text": "Papercut"
  },
  {
    "objectID": "posts/007_midjourney_experiments/midjourney_experiments.html#in-the-style-of",
    "href": "posts/007_midjourney_experiments/midjourney_experiments.html#in-the-style-of",
    "title": "Midjourney Experiments",
    "section": "In the style of …",
    "text": "In the style of …\n\nI’ll keep adding to the Neat Prompt Tags section."
  },
  {
    "objectID": "posts/007_midjourney_experiments/midjourney_experiments.html#footnotes",
    "href": "posts/007_midjourney_experiments/midjourney_experiments.html#footnotes",
    "title": "Midjourney Experiments",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nIan Goodfellow et. al’s paper (2014) presenting the Generative Adversarial Network (or GAN) deep learning model architecture.↩︎\nGoogle’s famous Attention is all you need paper (2017) which presented the Transformer deep learning model architecture.↩︎\nOpenAI’s 2018 analysis of available computing power (or “compute”) and the amount of compute needed to train models.↩︎\nGoogle’s “Transformer: A Novel Neural Network Architecture for Language Understanding” blog post (2017) outlining how Google used Transformers to generate more accurate translations in Google Translate.↩︎\nBefore or on Feb 11, 2019, This Person Does Not Exist came online, generating a hyperrealistic but fake image of a human face.↩︎\nIn late 2020, DeepMind (a subsidiary of Alphabet) announced AlphaFold2 a reengineered version of their protein-structure prediction system, which uses transformers to predict much more accurate representations of proteins. Understanding the molecular structure of complex proteins makes it possible to determine what molecular groups are exposed on the outer surface of the molecule, which is useful as only those exposed groups can interact with groups on other molecules.↩︎\nIn late 2020, uberduck.ai introduced a text-to-speech generation service and (per Uberduck copy) over 5000 voice models that can be used to generate speech from text. In early to mid 2021, this platform gained some press when users started generating not-quite-passable tracks with different rapper voice models.↩︎\nHugging created spaces where users could host small AI applications using existing transformer models on their platform.↩︎\nassuming the use is not for commercial purposes by a company earning over $1M in gross refenue per year, which must purchase the Pro (\\$60/mo) or Mega (\\$120/mo) plan. Source↩︎\nA lawsuit↩︎"
  }
]