[
  {
    "objectID": "posts/002_notebook_test/jupyter_quarto.html",
    "href": "posts/002_notebook_test/jupyter_quarto.html",
    "title": "Working with Notebooks",
    "section": "",
    "text": "Code\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nr = np.arange(0, 4, 0.01)\ntheta = 2 * np.pi * r\nfig, ax = plt.subplots(\n  subplot_kw = {'projection': 'polar'} \n)\nax.plot(theta, r)\nax.set_rticks([0.5, 1, 1.5, 2])\nax.grid(True)\nplt.show()\n\n\n\n\n\nFigure 1: A line plot on a polar axis\n\n\n\n\n\nDevelop a notebook\nRender that notebook via\n(quarto_env) user@hostname:~/...$ quarto render notebook_name.ipynb\nPreview your document via\n(quarto_env) user@hostname:~/...$ quarto preview quarto_blog/\nNote: Make sure draft: false in your document, or it won’t render.\n\n\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/006_great_expectations_setup/data_monitoring_w_gx_1_setup.html",
    "href": "posts/006_great_expectations_setup/data_monitoring_w_gx_1_setup.html",
    "title": "Data Quality Monitoring with Great Expectations",
    "section": "",
    "text": "Imports and path-definition\nfrom collections import Counter\nimport datetime as dt\nfrom pathlib import Path\nfrom urllib.request import urlretrieve\n\nimport geopandas as gpd\nimport pandas as pd\n\nPROJECT_DIR = Path(\".\").resolve()\nPROJECT_DATA_DIR = PROJECT_DIR.joinpath(\"data\")\nCode\n!rm {PROJECT_DATA_DIR.joinpath(\"food_inspection*\")}\n!rm -r {PROJECT_DIR.joinpath(\"great_expectations\")}\nGreat Expectations (or GX for short) is an open-source Python-based library that brings the idea of “testing” to your data. It enables you to define expectations for properties of your datasets (like records per batch, distribution of values in a column, columns in a table, etc) and check that the data meets those expectations when the data is updated."
  },
  {
    "objectID": "posts/006_great_expectations_setup/data_monitoring_w_gx_1_setup.html#step-0-great-expectations-setup",
    "href": "posts/006_great_expectations_setup/data_monitoring_w_gx_1_setup.html#step-0-great-expectations-setup",
    "title": "Data Quality Monitoring with Great Expectations",
    "section": "Step 0: Great Expectations Setup",
    "text": "Step 0: Great Expectations Setup\nFirst, you’ll need to install the great_expectations. If you already have conda installed on your machine, you can easily set up a conda env just like the one used to run this notebook by: 1. copying the gx_env_environment.yml file in the same dir as this notebook file to your machine, 2. open a terminal and navigate to the dir with that new file, and 3. run command conda env create -f environment.yml\n\n\nCollecting and preprocessing sample data for this post\nPROJECT_DATA_DIR.mkdir(exist_ok=True)\n\n# First, we need to download the data to our local machine.\nurl = \"https://data.cityofchicago.org/api/geospatial/4ijn-s7e5?method=export&format=GeoJSON\"\nfull_file_path = PROJECT_DATA_DIR.joinpath(\"full_food_inspections.geojson\")\nif not full_file_path.is_file():\n    urlretrieve(url=url, filename=full_file_path)\nfood_inspection_gdf = gpd.read_file(full_file_path)\n\n# For some reason, Socrata adds on these four always-null location columns on\n#   to geospatial exports. I'm going to remove them.\nlocation_cols = [\"location_state\", \"location_zip\", \"location_address\", \"location_city\"]\n# uncomment the lines below to confirm those columns are always empty\n# print(\"Rows with a non-null value in these location_xxx columns:\")\n# display(food_inspection_gdf[location_cols].notnull().sum())\nfood_inspection_gdf = food_inspection_gdf.drop(columns=location_cols)\n\n# That column ordering is a bit chaotic, so I'll reorder them (for readability).\ncol_order = [\n    \"inspection_id\", \"inspection_date\", \"dba_name\", \"aka_name\", \"license_\", \"facility_type\",\n    \"risk\", \"inspection_type\", \"results\", \"address\", \"city\", \"state\", \"zip\", \"violations\",\n    \"longitude\", \"latitude\", \"geometry\"\n]\nfood_inspection_gdf = food_inspection_gdf[col_order].copy()\n\n# I also want to break this into batches based on the dates, so I need to cast\n#   the `inspection_date` to a datetime type.\nfood_inspection_gdf[\"inspection_date\"] = pd.to_datetime(\n    food_inspection_gdf[\"inspection_date\"]\n)\n\n# I'll also cast string and numeric features to their proper dtypes.\n# food_inspection_gdf = food_inspection_gdf.convert_dtypes()\nfood_inspection_gdf[\"inspection_id\"] = food_inspection_gdf[\"inspection_id\"].astype(\"Int64\")\nfood_inspection_gdf[\"longitude\"] = food_inspection_gdf[\"longitude\"].astype(float)\nfood_inspection_gdf[\"latitude\"] = food_inspection_gdf[\"latitude\"].astype(float)\n\n# I'll also just make all string uppercase (to reduce cardinality)\nstr_cols = list(food_inspection_gdf.head(2).select_dtypes(include=\"object\").columns)\nfood_inspection_gdf[str_cols] = food_inspection_gdf[str_cols].apply(lambda x: x.str.upper())\n\n\nIn the (folded up) cell below, we split the dataset into batches and write each batch to file in this post’s ./data directory.\n\n\nAnd here we split the dataset into batches and write each batch to file in this post’s ./data directory.\n# I want to split the data into 1-month batches, so I need to get the first day of the month\n#   for every month between the earliest inspection and the month after the latest inspection\n#   in our food inspection dataset.\nmonth_start_dates = pd.date_range(\n    start=food_inspection_gdf[\"inspection_date\"].min() + pd.DateOffset(months=-1),\n    end=food_inspection_gdf[\"inspection_date\"].max(),\n    freq=\"MS\",\n)\n\n# Here, we'll iterate through each of those month_start_dates, extract the batch of data,\n#   format a filename containing the month_start_date, and write the batch to file.\nfor month_start_date in month_start_dates:\n    batch_period = pd.to_datetime(month_start_date).strftime(\"%Y_%m\")\n    batch_data = food_inspection_gdf.loc[\n        food_inspection_gdf[\"inspection_date\"].between(\n            left=month_start_date,\n            right=month_start_date + pd.DateOffset(months=1),\n            inclusive=\"left\")\n    ].copy()\n    batch_file_path = PROJECT_DATA_DIR.joinpath(f\"food_inspection_batch_{batch_period}.parquet\")\n    if not batch_file_path.is_file():\n        batch_data.to_parquet(batch_file_path, index=False)"
  },
  {
    "objectID": "posts/006_great_expectations_setup/data_monitoring_w_gx_1_setup.html#step-1-create-or-load-great-expectations-datacontext",
    "href": "posts/006_great_expectations_setup/data_monitoring_w_gx_1_setup.html#step-1-create-or-load-great-expectations-datacontext",
    "title": "Data Quality Monitoring with Great Expectations",
    "section": "Step 1: Create or Load Great Expectations DataContext",
    "text": "Step 1: Create or Load Great Expectations DataContext\nA DataContext is your primary tool for configuring your project and accessing project resources or GX methods. When you first create a DataContext for your project, GX will create a directory named /great_expectations in the project_root_dir directory.\nThe code below will create a new DataContext if one doesn’t already exist in the PROJECT_DIR directory, and then load a DataContext instance from that PROJECT_DIR. Great Expectations defaults to collecting anonymized usage statistics, but you can disable that for your context by setting usage_statistics_enabled=False.\n\nimport great_expectations as gx\nfrom great_expectations.data_context import FileDataContext\n\ncontext = FileDataContext.create(project_root_dir=PROJECT_DIR, usage_statistics_enabled=False)\n\nThis tutorial uses a local FileDataContext, but GX also supports CloudDataContexts and EphemeralDataContexts.\n\n\nKinds of DataContexts\n[el for el in dir(gx.data_context) if el.endswith(\"Context\")]\n\n\n['AbstractDataContext',\n 'BaseDataContext',\n 'CloudDataContext',\n 'DataContext',\n 'EphemeralDataContext',\n 'FileDataContext']"
  },
  {
    "objectID": "posts/006_great_expectations_setup/data_monitoring_w_gx_1_setup.html#step-2-create-or-load-a-datasource",
    "href": "posts/006_great_expectations_setup/data_monitoring_w_gx_1_setup.html#step-2-create-or-load-a-datasource",
    "title": "Data Quality Monitoring with Great Expectations",
    "section": "Step 2: Create or load a Datasource",
    "text": "Step 2: Create or load a Datasource\nA GX Datasource connects you to a source of data and gives you methods to define and access DataAssets.\nThe code below will check the DataContext context for a Datasource with the given datasource_name, and either load or create a local filesystem Datasource instance.\n\ndatasource_name = \"food_inspection_datasource\"\n\nif any(el[\"name\"] == datasource_name for el in context.list_datasources()):\n    print(f\"Datasource with name '{datasource_name}' found; loading now\")\n    datasource = context.get_datasource(datasource_name)\nelse:\n    print(f\"No Datasource with name '{datasource_name}' found; creating now\")\n    datasource = context.sources.add_pandas_filesystem(\n        name=datasource_name,\n        base_directory=PROJECT_DATA_DIR\n    )\n\nNo Datasource with name 'food_inspection_datasource' found; creating now\n\n\n\n\nOther kinds of GX Datasources\n[el for el in dir(context.sources) if el.startswith(\"add_\") and \"_update_\" not in el]\n\n\n['add_pandas',\n 'add_pandas_abs',\n 'add_pandas_dbfs',\n 'add_pandas_filesystem',\n 'add_pandas_gcs',\n 'add_pandas_s3',\n 'add_postgres',\n 'add_spark',\n 'add_spark_abs',\n 'add_spark_dbfs',\n 'add_spark_filesystem',\n 'add_spark_gcs',\n 'add_spark_s3',\n 'add_sql',\n 'add_sqlite']"
  },
  {
    "objectID": "posts/006_great_expectations_setup/data_monitoring_w_gx_1_setup.html#step-3-define-dataassets-in-that-datasource",
    "href": "posts/006_great_expectations_setup/data_monitoring_w_gx_1_setup.html#step-3-define-dataassets-in-that-datasource",
    "title": "Data Quality Monitoring with Great Expectations",
    "section": "Step 3: Define DataAssets in that Datasource",
    "text": "Step 3: Define DataAssets in that Datasource\nA GX DataAsset specifies a collection of records in a Datasource and the method for accessing those records.\nThe code below checks if a DataAsset with the given name exists in the datasource, loading it if it exists, or specifying it if not. In the part that specifies the DataAsset, note that we set the name of the asset, specify that the data is in parquet files, and provide a regex pattern for the file_names and also defines variable-names for the year and month parts each file_name. We can use those year and month variables to specify how DataAssets should be split into batches and the order of those batches.\n\ndata_asset_name = \"food_inspections_asset\"\n\nif data_asset_name not in datasource.get_asset_names():\n    print(f\"Creating data asset {data_asset_name}\")\n    data_asset = datasource.add_parquet_asset(\n        name=data_asset_name,\n        batching_regex = r\"food_inspection_batch_(?P&lt;year&gt;\\d{4})_(?P&lt;month&gt;\\d{2})\\.parquet\"\n    )\nelse:\n    data_asset = datasource.get_asset(data_asset_name)\ndata_asset = data_asset.add_sorters([\"+year\", \"+month\"])\n\nCreating data asset food_inspections_asset\n\n\n\n\nOther data file formats GX supports\n[el for el in dir(datasource) if el.startswith(\"add_\")]\n\n\n['add_csv_asset',\n 'add_excel_asset',\n 'add_feather_asset',\n 'add_fwf_asset',\n 'add_hdf_asset',\n 'add_html_asset',\n 'add_json_asset',\n 'add_orc_asset',\n 'add_parquet_asset',\n 'add_pickle_asset',\n 'add_sas_asset',\n 'add_spss_asset',\n 'add_stata_asset',\n 'add_xml_asset']"
  },
  {
    "objectID": "posts/006_great_expectations_setup/data_monitoring_w_gx_1_setup.html#step-4-create-expectations-for-a-dataasset",
    "href": "posts/006_great_expectations_setup/data_monitoring_w_gx_1_setup.html#step-4-create-expectations-for-a-dataasset",
    "title": "Data Quality Monitoring with Great Expectations",
    "section": "Step 4: Create Expectations for a DataAsset",
    "text": "Step 4: Create Expectations for a DataAsset\nA GX Expectation is a verifiable assertion about some property of a DataAsset, and defining Expectations both enables GX to check that data meets expectations and enables domain experts to explicitly represent and communicate Expectations for data.\nGX supports hundreds of different Expectations and catalogs them in the Expectation Gallery (although not all Expectations are implemented for all kinds of Datasources). GX also provides tools to aid in several workflows for defining suites of Expectations, including the GX Data Assistant workflow (used below), which builds a suite of Expectations by profiling batches of data.\nIn the code below, we create a new Expectation suite (on lines 3-5), organize batches of data (on lines 6-7), and use the data assistant to profile the DataAsset based on our batches of data (on lines 8-11).\n\nexpectation_suite_name = \"food_inspections_suite\"\n\nexpectation_suite = context.add_or_update_expectation_suite(\n    expectation_suite_name=expectation_suite_name\n)\nbatch_request = data_asset.build_batch_request()\nbatches = data_asset.get_batch_list_from_batch_request(batch_request)\ndata_assistant_result = context.assistants.onboarding.run(\n    batch_request=batch_request,\n    exclude_column_names=[\"inspection_date\", \"geometry\"],\n)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThe profiler will sequentially generate a lot of progress bars (like these) as it profiles dataset features.\n\n\n\nProfiler output\n\n\n\ndata_assistant_result.plot_expectations_and_metrics()\n\n64 Expectations produced, 23 Expectation and Metric plots implemented\nUse DataAssistantResult.show_expectations_by_domain_type() or\nDataAssistantResult.show_expectations_by_expectation_type() to show all produced Expectations\n\n\n\n                \n                \n\n\n\n                \n            \n\n\n\n                \n            \n\n\n\n\n\n\n\n\n\nData Assistant Plot Inspector plots\nAfter the Data Assistant finishes profiling, it outputs results to a variable we named data_assistant_result, and you can explore the results across batches by calling data_assistant_result.plot_expectations_and_metrics() and selecting the expectation and column you’re interested in.\n   \n\n\nExtracting, [optionally] Editing, and Committing our Expectation Suite to our DataContext\nIf we’re content with the Expectations generated by the Data Assistant’s profiler, we can simply extract the Expectations and add them to our context via\nexpectation_suite = data_assistant_result.get_expectation_suite(\n    expectation_suite_name=expectation_suite_name\n)\nsaved_suite = context.add_or_update_expectation_suite(expectation_suite=expectation_suite)\nIn a future post I’ll go into further depth on methods for editing Expectations, but here I’ll show how to inspect and remove Expectations.\n\nexpectation_suite = data_assistant_result.get_expectation_suite(\n    expectation_suite_name=expectation_suite_name\n)\n\n\n\nCounts of Expectations by Column\nprint(f\"Counts of Expectations by Expectation-type:\")\nexpecs_by_type = expectation_suite.get_grouped_and_ordered_expectations_by_expectation_type()\ndisplay(Counter([ex._expectation_type for ex in expecs_by_type]))\n\nprint(f\"\\n\\nCounts of Expectations by Column-name:\")\nexpecs_by_col = expectation_suite.get_grouped_and_ordered_expectations_by_column()\nexpec_count_by_col = {col: len(col_expecs) for col, col_expecs in expecs_by_col[0].items()}\nsorted(expec_count_by_col.items(), key=lambda x: x[1], reverse=True)\n\n\nCounts of Expectations by Expectation-type:\n\n\nCounts of Expectations by Column-name:\n\n\nCounter({'expect_column_value_lengths_to_be_between': 12,\n         'expect_column_values_to_match_regex': 11,\n         'expect_column_proportion_of_unique_values_to_be_between': 7,\n         'expect_column_unique_value_count_to_be_between': 7,\n         'expect_column_values_to_be_in_set': 7,\n         'expect_column_values_to_not_be_null': 4,\n         'expect_column_max_to_be_between': 2,\n         'expect_column_mean_to_be_between': 2,\n         'expect_column_median_to_be_between': 2,\n         'expect_column_min_to_be_between': 2,\n         'expect_column_quantile_values_to_be_between': 2,\n         'expect_column_stdev_to_be_between': 2,\n         'expect_column_values_to_be_between': 2,\n         'expect_table_columns_to_match_set': 1,\n         'expect_table_row_count_to_be_between': 1})\n\n\n[('longitude', 7),\n ('latitude', 7),\n ('inspection_type', 6),\n ('results', 6),\n ('facility_type', 5),\n ('risk', 5),\n ('city', 5),\n ('state', 5),\n ('zip', 5),\n ('dba_name', 3),\n ('_nocolumn', 2),\n ('address', 2),\n ('aka_name', 2),\n ('license_', 2),\n ('violations', 2)]\n\n\n\n\nExpectation-related methods on our expectation_suite\n[el for el in dir(expectation_suite) if \"_expectation\" in el]\n\n\n['_add_expectation',\n '_get_expectations_by_domain_using_accessor_method',\n '_validate_expectation_configuration_before_adding',\n 'add_expectation',\n 'add_expectation_configurations',\n 'append_expectation',\n 'find_expectation_indexes',\n 'find_expectations',\n 'get_column_expectations',\n 'get_column_pair_expectations',\n 'get_grouped_and_ordered_expectations_by_column',\n 'get_grouped_and_ordered_expectations_by_domain_type',\n 'get_grouped_and_ordered_expectations_by_expectation_type',\n 'get_multicolumn_expectations',\n 'get_table_expectations',\n 'patch_expectation',\n 'remove_all_expectations_of_type',\n 'remove_expectation',\n 'replace_expectation',\n 'show_expectations_by_domain_type',\n 'show_expectations_by_expectation_type']\n\n\n\n\nInspecting Expectation types for a given column\ncol_name = \"longitude\"\n[ex[\"expectation_type\"] for ex in expectation_suite.get_column_expectations() if ex[\"kwargs\"][\"column\"] == col_name]\n\n\n['expect_column_min_to_be_between',\n 'expect_column_max_to_be_between',\n 'expect_column_values_to_be_between',\n 'expect_column_quantile_values_to_be_between',\n 'expect_column_median_to_be_between',\n 'expect_column_mean_to_be_between',\n 'expect_column_stdev_to_be_between']\n\n\nSome Expectations are redundant, such as expect_column_min_to_be_between and expect_column_max_to_be_between. I’ll remove them.\n\ncol_name = \"longitude\"\nexpectation_types_to_remove = [\n    \"expect_column_min_to_be_between\", \"expect_column_max_to_be_between\"\n]\ncol_expectations_w_type = [\n    ex for ex in expectation_suite.get_column_expectations()\n    if (ex[\"kwargs\"][\"column\"] == col_name) and (ex[\"expectation_type\"] in expectation_types_to_remove)\n]\ncol_expectations_w_type\n\n[{\"meta\": {\"profiler_details\": {\"metric_configuration\": {\"metric_name\": \"column.min\", \"domain_kwargs\": {\"column\": \"longitude\"}, \"metric_value_kwargs\": null}, \"num_batches\": 162}}, \"expectation_type\": \"expect_column_min_to_be_between\", \"kwargs\": {\"column\": \"longitude\", \"max_value\": -87.81649552747085, \"strict_min\": false, \"strict_max\": false, \"min_value\": -87.91442843927047}},\n {\"meta\": {\"profiler_details\": {\"metric_configuration\": {\"metric_name\": \"column.max\", \"domain_kwargs\": {\"column\": \"longitude\"}, \"metric_value_kwargs\": null}, \"num_batches\": 162}}, \"expectation_type\": \"expect_column_max_to_be_between\", \"kwargs\": {\"column\": \"longitude\", \"max_value\": -87.5250941359867, \"strict_min\": false, \"strict_max\": false, \"min_value\": -87.5510612280602}}]\n\n\n\nprint(f\"Expectations prior to removal: {len(expectation_suite.expectations)}\")\nfor expectation_to_remove in col_expectations_w_type:\n    removed_expec = expectation_suite.remove_expectation(expectation_to_remove)\nprint(f\"Expectations after removal:    {len(expectation_suite.expectations)}\")\n\nExpectations prior to removal: 64\nExpectations after removal:    62\n\n\nWe can also get rid of every instance of an Expectation type.\n\nprint(f\"Removing Expectation types:\")\nfor expec_type in expectation_types_to_remove:\n    print(f\"  - {expec_type}\")\nprint(f\"Expectations prior to removal: {len(expectation_suite.expectations)}\")\nremoved_expecs = expectation_suite.remove_all_expectations_of_type(expectation_types=expectation_types_to_remove)\nprint(f\"Expectations after removal:    {len(expectation_suite.expectations)}\")\n\nRemoving Expectation types:\n  - expect_column_min_to_be_between\n  - expect_column_max_to_be_between\nExpectations prior to removal: 62\nExpectations after removal:    60\n\n\nAfter reviewing and editing Expectations, the Expectation Suite must be committed to the DataContext.\n\nsaved_suite = context.add_or_update_expectation_suite(expectation_suite=expectation_suite)"
  },
  {
    "objectID": "posts/006_great_expectations_setup/data_monitoring_w_gx_1_setup.html#step-5-setup-a-checkpoint-to-check-expectations",
    "href": "posts/006_great_expectations_setup/data_monitoring_w_gx_1_setup.html#step-5-setup-a-checkpoint-to-check-expectations",
    "title": "Data Quality Monitoring with Great Expectations",
    "section": "Step 5: Setup a Checkpoint to check Expectations",
    "text": "Step 5: Setup a Checkpoint to check Expectations\nA GX Checkpoint configures the validation process for an Expectation Suite.\nSpecifically, a Checkpoint defines: * the Expectation Suite to evaluate, * the data Batches to evaluate against, and * the actions to take after evaluation.\nWe’ll take the actions of compiling a report of results and committing our Checkpoint to our DataContext, but you can also configure a Checkpoint to send results via a email or Slack notification.\n\ncheckpoint_name = \"food_inspections_checkpoint\"\n\ncheckpoint = gx.checkpoint.SimpleCheckpoint(\n    name=checkpoint_name,\n    data_context=context,\n    validations=[\n        {\n            \"batch_request\": batch_request,\n            \"expectation_suite_name\": expectation_suite_name,\n        },\n    ],\n)\ncheckpoint_result = checkpoint.run()\n\n\n\n\n\ncontext.build_data_docs()\n\n{'local_site': 'file:///home/matt/projects/blogs/quarto_blog/posts/006_great_expectations_setup/great_expectations/uncommitted/data_docs/local_site/index.html'}\n\n\nTo view the generated validation report (or Data Docs), open the file .../great_expectations/uncommitted/data_docs/local_site/index.html and select the validation run you want to review. You may have to click Trust HTML (upper left corner in Jupyterlab) to navigate the document.\n \n\ncontext.add_checkpoint(checkpoint=checkpoint)\n\n{\n  \"action_list\": [\n    {\n      \"name\": \"store_validation_result\",\n      \"action\": {\n        \"class_name\": \"StoreValidationResultAction\"\n      }\n    },\n    {\n      \"name\": \"store_evaluation_params\",\n      \"action\": {\n        \"class_name\": \"StoreEvaluationParametersAction\"\n      }\n    },\n    {\n      \"name\": \"update_data_docs\",\n      \"action\": {\n        \"class_name\": \"UpdateDataDocsAction\"\n      }\n    }\n  ],\n  \"batch_request\": {},\n  \"class_name\": \"SimpleCheckpoint\",\n  \"config_version\": 1.0,\n  \"evaluation_parameters\": {},\n  \"module_name\": \"great_expectations.checkpoint\",\n  \"name\": \"food_inspections_checkpoint\",\n  \"profilers\": [],\n  \"runtime_configuration\": {},\n  \"validations\": [\n    {\n      \"batch_request\": {\n        \"datasource_name\": \"food_inspection_datasource\",\n        \"data_asset_name\": \"food_inspections_asset\",\n        \"options\": {}\n      },\n      \"expectation_suite_name\": \"food_inspections_suite\"\n    }\n  ]\n}\n\n\nYou can easily integrate a defined Checkpoint into a pipeline with just a few lines of code (and another dependency).\n\nimport great_expectations as gx\n\ncontext = gx.get_context(context_root_dir=PROJECT_DIR.joinpath(\"great_expectations\"))\nretrieved_checkpoint = context.get_checkpoint(name=\"food_inspections_checkpoint\")\nretrieved_checkpoint_result = retrieved_checkpoint.run()\nif not retrieved_checkpoint_result[\"success\"]:\n    print(f\"Failed Validation Checkpoint!\")\n    # or raise Exception(\"if you'd rather handle validation failures that way\")"
  },
  {
    "objectID": "posts/006_great_expectations_setup/data_monitoring_w_gx_1_setup.html#next-steps",
    "href": "posts/006_great_expectations_setup/data_monitoring_w_gx_1_setup.html#next-steps",
    "title": "Data Quality Monitoring with Great Expectations",
    "section": "Next Steps",
    "text": "Next Steps\nIn future posts, I’ll dive deeper into the Expectation-setting process, demonstrate a workflow with a PostgreSQL Datasource and DataAssets (where GX really shines), and explore strategies for integrating data monitoring into production ETL/ELT pipelines (like those in my personal data warehousing platform)."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "quarto_blog",
    "section": "",
    "text": "Data Quality Monitoring with Great Expectations\n\n\n\n\n\n\n\ngreat_expectations\n\n\ntutorial\n\n\nlong\n\n\n\n\nPart 1: Setup and Setting Expectations\n\n\n\n\n\n\nJun 23, 2023\n\n\nMatt Triano\n\n\n\n\n\n\n  \n\n\n\n\nCensus TIGER Dataset Collector Dev\n\n\n\n\n\nDocumenting the synthesis stage of development\n\n\n\n\n\n\nJun 14, 2023\n\n\nMatt Triano\n\n\n\n\n\n\n  \n\n\n\n\nWorking with Notebooks\n\n\n\n\n\nHow to develop a post in a notebook\n\n\n\n\n\n\nJun 12, 2023\n\n\nMatt Triano\n\n\n\n\n\n\n  \n\n\n\n\nSetting up Conda\n\n\n\n\n\n\n\nconda\n\n\nsetup\n\n\ntutorial\n\n\n\n\n\n\n\n\n\n\n\nJul 22, 2022\n\n\nMatt Triano\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "Matt Triano is a Data Scientist at the University of Chicago’s Urban Labs. When not innovating on data platforms, Matt enjoys exploring new tech, open source development, and hardware hacking.\n\n\nDePaul University | Chicago, IL M.S. in Computer Science | Sept 2013 - Mar 2018\nPurdue University | West Lafayette, IN B.S. in Physics | Aug 2006 - Dec 2010"
  },
  {
    "objectID": "about.html#education",
    "href": "about.html#education",
    "title": "About",
    "section": "",
    "text": "DePaul University | Chicago, IL M.S. in Computer Science | Sept 2013 - Mar 2018\nPurdue University | West Lafayette, IN B.S. in Physics | Aug 2006 - Dec 2010"
  },
  {
    "objectID": "posts/000_setting_up_miniconda/Setting_up_Miniconda.html",
    "href": "posts/000_setting_up_miniconda/Setting_up_Miniconda.html",
    "title": "Setting up Conda",
    "section": "",
    "text": "conda is a language-agnostic package manager and environment management system. conda’s environment management functionality makes it possible for a user to easily switch between environments (where an environment consists of the hardware and software used to execute code) and makes it possible to export a specification of that environment that can be used to reproduce that environment on another system.\nIn this post, I’ll show my opinionated conda installation and configuration process.\n\n\nconda is primarily installable via two distributions; the Anaconda distribution, which includes the conda executable along with over 700 additional conda packages from the Anaconda repository, and the Miniconda distribution, which consists of the conda executable with the minimal number of packages needed for conda to run. Install a miniconda distribution.\n\nDownload a miniconda installer, copy the corresponding SHA256 hash, calculate the SHA256 hash of the downloaded file, and compare that value to the one copied from the download link page. If the values match exactly, proceed in installation.\n\nFor the miniconda version downloaded below, the SHA256 sum should be aef279d6baea7f67940f16aad17ebe5f6aac97487c7c03466ff01f4819e5a651\n\n\n\n\nCode\n!curl -o Miniconda3-py310_23.3.1-0-Linux-x86_64.sh https://repo.anaconda.com/miniconda/Miniconda3-py310_23.3.1-0-Linux-x86_64.sh\n\n\n  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n                                 Dload  Upload   Total   Spent    Left  Speed\n100 69.7M  100 69.7M    0     0  75.9M      0 --:--:-- --:--:-- --:--:-- 75.8M\n\n\n\n\nCode\n!sha256sum Miniconda3-py310_23.3.1-0-Linux-x86_64.sh\n\n\naef279d6baea7f67940f16aad17ebe5f6aac97487c7c03466ff01f4819e5a651  Miniconda3-py310_23.3.1-0-Linux-x86_64.sh\n\n\n\nExecute the installer."
  },
  {
    "objectID": "posts/000_setting_up_miniconda/Setting_up_Miniconda.html#conda-installation",
    "href": "posts/000_setting_up_miniconda/Setting_up_Miniconda.html#conda-installation",
    "title": "Setting up Conda",
    "section": "",
    "text": "conda is primarily installable via two distributions; the Anaconda distribution, which includes the conda executable along with over 700 additional conda packages from the Anaconda repository, and the Miniconda distribution, which consists of the conda executable with the minimal number of packages needed for conda to run. Install a miniconda distribution.\n\nDownload a miniconda installer, copy the corresponding SHA256 hash, calculate the SHA256 hash of the downloaded file, and compare that value to the one copied from the download link page. If the values match exactly, proceed in installation.\n\nFor the miniconda version downloaded below, the SHA256 sum should be aef279d6baea7f67940f16aad17ebe5f6aac97487c7c03466ff01f4819e5a651\n\n\n\n\nCode\n!curl -o Miniconda3-py310_23.3.1-0-Linux-x86_64.sh https://repo.anaconda.com/miniconda/Miniconda3-py310_23.3.1-0-Linux-x86_64.sh\n\n\n  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n                                 Dload  Upload   Total   Spent    Left  Speed\n100 69.7M  100 69.7M    0     0  75.9M      0 --:--:-- --:--:-- --:--:-- 75.8M\n\n\n\n\nCode\n!sha256sum Miniconda3-py310_23.3.1-0-Linux-x86_64.sh\n\n\naef279d6baea7f67940f16aad17ebe5f6aac97487c7c03466ff01f4819e5a651  Miniconda3-py310_23.3.1-0-Linux-x86_64.sh\n\n\n\nExecute the installer."
  },
  {
    "objectID": "posts/004_census_tiger_dev/TIGER_data_collector_dev.html",
    "href": "posts/004_census_tiger_dev/TIGER_data_collector_dev.html",
    "title": "Census TIGER Dataset Collector Dev",
    "section": "",
    "text": "Code\nimport textwrap\nfrom typing import Dict, List, Union, Optional\nfrom urllib.request import urlretrieve\n\nfrom bs4 import BeautifulSoup\nimport geopandas as gpd\nimport matplotlib.pyplot as plt\nimport pandas as pd\nimport requests\n\n\nThe US Census Bureau (USCB) runs a massive array of surveys of residents of the United States. After aggregating responses into demographic and/or geographic groups, the USCB publishes those aggregated datasets to its massive public data catalog. I’ve already developed some tooling to collect and ingest Census datasets into my personal data warehouse and analytics environment, but to map out geographic Census data or join in other geospatial datasets, I also need to collect and ingest spatial files describing the geographic boundaries of Census groupings. Fortunately, the USCB provides shapefiles with these geometries in their TIGER (Topologically Integrated Geographic Encoding and Referencing) data offerings, and in this notebook, I’ll synthesize the findings the experiments and research in my last notebook into a data model and data collection tools.\n\nReferences:\n\nTIGER Technical Docs\nTIGER Geographic Codes\n\nEach year, the USCB can add or alter geographic boundaries or connected data features, so they organize TIGER data offering into annual vintages, which can be downloaded either through the USCB web interface or from their file server interface (https://www2.census.gov/geo/tiger/).\nThe functions in the cell below aid in scraping the table off of any of pages linked in that file-tree interface.\n\n\nCode\ndef request_page(metadata_url: str) -&gt; requests.models.Response:\n    resp = requests.get(metadata_url)\n    if resp.status_code == 200:\n        return resp\n    else:\n        raise Exception(f\"Couldn't get page metadata for url {metadata_url}\")\n\ndef scrape_census_ftp_metadata_page(metadata_url: str) -&gt; pd.DataFrame:\n    resp = request_page(metadata_url=metadata_url)\n    soup = BeautifulSoup(resp.content, \"html.parser\")\n    table = soup.find(\"table\")\n    rows = table.find_all(\"tr\")\n    table_contents = []\n    for row in rows:\n        cols = row.find_all(\"td\")\n        cols = [col.text.strip() for col in cols]\n        table_contents.append(cols)\n    table_rows = [el for el in table_contents if len(el) &gt; 0]\n\n    metadata_df = pd.DataFrame(\n        [row[1:] for row in table_rows],\n        columns=[\"name\", \"last_modified\", \"size\", \"description\"],\n    )\n    metadata_df[\"last_modified\"] = pd.to_datetime(metadata_df[\"last_modified\"])\n    metadata_df[\"is_dir\"] = metadata_df[\"name\"].str.endswith(\"/\")\n    metadata_df[\"clean_name\"] = metadata_df[\"name\"].str.replace(\"/$\", \"\", regex=True)\n    metadata_df[\"is_file\"] = (~metadata_df[\"is_dir\"]) & (\n        metadata_df[\"clean_name\"] != \"Parent Directory\"\n    )\n    while metadata_url.strip().endswith(\"/\"):\n        metadata_url = metadata_url[:-1]\n    mask = metadata_df[\"is_file\"] | metadata_df[\"is_dir\"]\n    metadata_df = metadata_df.loc[mask].copy()\n    metadata_df[\"metadata_url\"] = (metadata_url + \"/\" + metadata_df[\"clean_name\"])\n    return metadata_df\n\n\nrequest_page() makes a GET request for the content at the URL metadata_url and returns the response (if successful, i.e. if the response has the “success” HTTP response status code). This function is only called by scrape_census_ftp_metadata_page(), but it’s separated out to make it easier to test scrape_census_ftp_metadata_page().\n\n\nCode\ndef request_page(metadata_url: str) -&gt; requests.models.Response:\n    resp = requests.get(metadata_url)\n    if resp.status_code == 200:\n        return resp\n    else:\n        raise Exception(f\"Couldn't get page metadata for url {metadata_url}\")\n\n\nscrape_census_ftp_metadata_page() gets the content from a given Census file-tree page (at URL metadata_url), parses it into a convenient and well structured format (a pandas DataFrame) with features that aid in filtering to desired rows.\n\n\nCode\nresp = request_page(metadata_url=\"https://www2.census.gov/geo/tiger/\")\nsoup = BeautifulSoup(resp.content, \"html.parser\")\ntable = soup.find(\"table\")\nrows = table.find_all(\"tr\")\nrows[0:7]\n\n\n[&lt;tr&gt;&lt;th valign=\"top\"&gt;&lt;img alt=\"[ICO]\" src=\"/icons/blank.gif\"/&gt;&lt;/th&gt;&lt;th&gt;&lt;a href=\"?C=N;O=D\"&gt;Name&lt;/a&gt;&lt;/th&gt;&lt;th&gt;&lt;a href=\"?C=M;O=A\"&gt;Last modified&lt;/a&gt;&lt;/th&gt;&lt;th&gt;&lt;a href=\"?C=S;O=A\"&gt;Size&lt;/a&gt;&lt;/th&gt;&lt;th&gt;&lt;a href=\"?C=D;O=A\"&gt;Description&lt;/a&gt;&lt;/th&gt;&lt;/tr&gt;,\n &lt;tr&gt;&lt;th colspan=\"5\"&gt;&lt;hr/&gt;&lt;/th&gt;&lt;/tr&gt;,\n &lt;tr&gt;&lt;td valign=\"top\"&gt;&lt;img alt=\"[PARENTDIR]\" src=\"/icons/back.gif\"/&gt;&lt;/td&gt;&lt;td&gt;&lt;a href=\"/geo/\"&gt;Parent Directory&lt;/a&gt;&lt;/td&gt;&lt;td&gt; &lt;/td&gt;&lt;td align=\"right\"&gt;  - &lt;/td&gt;&lt;td&gt; &lt;/td&gt;&lt;/tr&gt;,\n &lt;tr&gt;&lt;td valign=\"top\"&gt;&lt;img alt=\"[   ]\" src=\"/icons/layout.gif\"/&gt;&lt;/td&gt;&lt;td&gt;&lt;a href=\"Directory_Contents_ReadMe.pdf\"&gt;Directory_Contents_ReadMe.pdf&lt;/a&gt;&lt;/td&gt;&lt;td align=\"right\"&gt;2019-06-25 09:13  &lt;/td&gt;&lt;td align=\"right\"&gt;439K&lt;/td&gt;&lt;td&gt; &lt;/td&gt;&lt;/tr&gt;,\n &lt;tr&gt;&lt;td valign=\"top\"&gt;&lt;img alt=\"[DIR]\" src=\"/icons/folder.gif\"/&gt;&lt;/td&gt;&lt;td&gt;&lt;a href=\"GENZ2010/\"&gt;GENZ2010/&lt;/a&gt;&lt;/td&gt;&lt;td align=\"right\"&gt;2013-07-24 12:46  &lt;/td&gt;&lt;td align=\"right\"&gt;  - &lt;/td&gt;&lt;td&gt; &lt;/td&gt;&lt;/tr&gt;,\n &lt;tr&gt;&lt;td valign=\"top\"&gt;&lt;img alt=\"[DIR]\" src=\"/icons/folder.gif\"/&gt;&lt;/td&gt;&lt;td&gt;&lt;a href=\"GENZ2012/\"&gt;GENZ2012/&lt;/a&gt;&lt;/td&gt;&lt;td align=\"right\"&gt;2013-07-24 12:47  &lt;/td&gt;&lt;td align=\"right\"&gt;  - &lt;/td&gt;&lt;td&gt; &lt;/td&gt;&lt;/tr&gt;,\n &lt;tr&gt;&lt;td valign=\"top\"&gt;&lt;img alt=\"[DIR]\" src=\"/icons/folder.gif\"/&gt;&lt;/td&gt;&lt;td&gt;&lt;a href=\"GENZ2013/\"&gt;GENZ2013/&lt;/a&gt;&lt;/td&gt;&lt;td align=\"right\"&gt;2014-07-02 08:28  &lt;/td&gt;&lt;td align=\"right\"&gt;  - &lt;/td&gt;&lt;td&gt; &lt;/td&gt;&lt;/tr&gt;]\n\n\nThe cell above shows code that extracts HTML tr (table-row) elements extracted from the page (sorry for showing something so ugly!), and the cell below shows the (beautiful) end product of scrape_census_ftp_metadata_page().\n\n\nCode\nall_tiger_vintages_df = scrape_census_ftp_metadata_page(\n    metadata_url=\"https://www2.census.gov/geo/tiger/\"\n)\ndisplay(all_tiger_vintages_df.head(4))\n\n\n\n\n\n\n\n\n\nname\nlast_modified\nsize\ndescription\nis_dir\nclean_name\nis_file\nmetadata_url\n\n\n\n\n1\nDirectory_Contents_ReadMe.pdf\n2019-06-25 09:13:00\n439K\n\nFalse\nDirectory_Contents_ReadMe.pdf\nTrue\nhttps://www2.census.gov/geo/tiger/Directory_Co...\n\n\n2\nGENZ2010/\n2013-07-24 12:46:00\n-\n\nTrue\nGENZ2010\nFalse\nhttps://www2.census.gov/geo/tiger/GENZ2010\n\n\n3\nGENZ2012/\n2013-07-24 12:47:00\n-\n\nTrue\nGENZ2012\nFalse\nhttps://www2.census.gov/geo/tiger/GENZ2012\n\n\n4\nGENZ2013/\n2014-07-02 08:28:00\n-\n\nTrue\nGENZ2013\nFalse\nhttps://www2.census.gov/geo/tiger/GENZ2013\n\n\n\n\n\n\n\n\n\nCode\nprint(f\"Files on the top-level TIGER dataset page:                {all_tiger_vintages_df['is_file'].sum():&gt;3}\")\nprint(f\"TIGER data offerings on the top-level TIGER dataset page: {all_tiger_vintages_df['is_dir'].sum():&gt;3}\")\n\n\nFiles on the top-level TIGER dataset page:                  1\nTIGER data offerings on the top-level TIGER dataset page:  59\n\n\nThe GENZyyyy TIGER data offerings are interesting, but the real wealth of geospatial files can be found in the TIGER offerings with names matching the TIGERyyyy pattern, and I implemented the get_tiger_vintages_metadata() function to get the metadata for these offerings, or vintages.\n\n\nCode\ndef get_tiger_vintages_metadata() -&gt; pd.DataFrame:\n    all_tiger_vintages_df = scrape_census_ftp_metadata_page(\n        metadata_url=\"https://www2.census.gov/geo/tiger/\"\n    )\n    tiger_vintages_df = all_tiger_vintages_df.loc[\n        all_tiger_vintages_df[\"name\"].str.contains(\"^TIGER\\d{4}/\", regex=True)\n    ].copy()\n    tiger_vintages_df = tiger_vintages_df.sort_values(by=\"name\", ignore_index=True)\n    return tiger_vintages_df\n\ntiger_vintages_df = get_tiger_vintages_metadata()\nprint(f\"Available TIGER vintages:\")\nnames = textwrap.wrap(\", \".join(list(tiger_vintages_df[\"clean_name\"].values)), width=95)\nfor line in names:\n    print(f\"    {line}\")\ndisplay(tiger_vintages_df.head(3))\n\n\nAvailable TIGER vintages:\n    TIGER1992, TIGER1999, TIGER2002, TIGER2003, TIGER2008, TIGER2009, TIGER2010, TIGER2011,\n    TIGER2012, TIGER2013, TIGER2014, TIGER2015, TIGER2016, TIGER2017, TIGER2018, TIGER2019,\n    TIGER2020, TIGER2021, TIGER2022\n\n\n\n\n\n\n\n\n\nname\nlast_modified\nsize\ndescription\nis_dir\nclean_name\nis_file\nmetadata_url\n\n\n\n\n0\nTIGER1992/\n2011-12-19 07:56:00\n-\n\nTrue\nTIGER1992\nFalse\nhttps://www2.census.gov/geo/tiger/TIGER1992\n\n\n1\nTIGER1999/\n2012-02-13 11:19:00\n-\n\nTrue\nTIGER1999\nFalse\nhttps://www2.census.gov/geo/tiger/TIGER1999\n\n\n2\nTIGER2002/\n2015-05-05 18:37:00\n-\n\nTrue\nTIGER2002\nFalse\nhttps://www2.census.gov/geo/tiger/TIGER2002\n\n\n\n\n\n\n\nNow that we have tools for scraping the any page in the Census’s file server, we can use those tools to retrieve data on the geographic entities in a given TIGER vintage.\n\n\nCode\nclass TIGERCatalog:\n    def __init__(self):\n        self.dataset_vintages = get_tiger_vintages_metadata()\n\n    def get_vintage_metadata(self, year: str) -&gt; pd.DataFrame:\n        return self.dataset_vintages.loc[self.dataset_vintages[\"name\"] == f\"TIGER{year}/\"].copy()\n\nclass TIGERVintageCatalog:\n    def __init__(self, year: str, catalog: Optional[TIGERCatalog] = None):\n        self.year = str(year)\n        self.set_catalog(catalog=catalog)\n\n    def set_catalog(self, catalog: Optional[TIGERCatalog]) -&gt; None:\n        if catalog is None:\n            self.catalog = TIGERCatalog()\n        else:\n            self.catalog = catalog\n\n    @property\n    def vintage_metadata(self):\n        return self.catalog.get_vintage_metadata(year=self.year)\n\n    @property\n    def vintage_entities(self):\n        if len(self.vintage_metadata) == 1:\n            tiger_vintage_url = self.vintage_metadata[\"metadata_url\"].values[0]\n            return scrape_census_ftp_metadata_page(metadata_url=tiger_vintage_url)\n        else:\n            raise Exception(\n                f\"Failed to get unambiguous metadata (got {self.vintage_metadata})\"\n            )\n\n    def get_entity_metadata(self, entity_name: str) -&gt; pd.DataFrame:\n        return self.vintage_entities.loc[self.vintage_entities[\"clean_name\"] == entity_name].copy()\n\n    def print_entity_names(self):\n        entity_names = self.vintage_entities.loc[\n            self.vintage_entities[\"is_dir\"], \"clean_name\"\n        ].values\n        print(f\"TIGER Entity options for the {self.year} TIGER vintage:\")\n        for entity_name in entity_names:\n            print(f\"  - {entity_name}\")\n        print(f\"Entity count: {len(entity_names)}\")\n\n\n\n\nCode\ntiger_catalog = TIGERCatalog()\nvintage_entity_catalog = TIGERVintageCatalog(year=\"2022\", catalog=tiger_catalog)\nvintage_entity_catalog.print_entity_names()\n\n\nTIGER Entity options for the 2022 TIGER vintage:\n  - ADDR\n  - ADDRFEAT\n  - ADDRFN\n  - AIANNH\n  - AITSN\n  - ANRC\n  - AREALM\n  - AREAWATER\n  - BG\n  - CD\n  - COASTLINE\n  - CONCITY\n  - COUNTY\n  - COUSUB\n  - EDGES\n  - ELSD\n  - ESTATE\n  - FACES\n  - FACESAH\n  - FACESAL\n  - FACESMIL\n  - FEATNAMES\n  - LINEARWATER\n  - MIL\n  - PLACE\n  - POINTLM\n  - PRIMARYROADS\n  - PRISECROADS\n  - PUMA\n  - RAILS\n  - ROADS\n  - SCSD\n  - SDADM\n  - SLDL\n  - SLDU\n  - STATE\n  - SUBBARRIO\n  - TABBLOCK20\n  - TBG\n  - TRACT\n  - TTRACT\n  - UAC\n  - UNSD\n  - ZCTA520\nEntity count: 44\n\n\nLet’s examine Census Tracts.\nI’ll need some tooling to collect information on a given entity in a given TIGER vintage. I know this object will need data that’s in the relevant TIGERVintageCatalog instance, so I’ll make that an attribute of the entity vintage class. I could require that the use passes in a TIGERVintageCatalog instance (which would reduce the number of calls to the same Census resource in the usecase where a user is interactively working using these classes), but ultimately I’m going to build out pipelines that only collect one TIGER entity at a from a given vintage (as I don’t need most of the available entities listed above).\n\n\nCode\nclass TIGERGeographicEntityVintage:\n    def __init__(self, entity_name: str, year: str, catalog: Optional[TIGERCatalog] = None):\n        self.entity_name = entity_name\n        self.year = str(year)\n        self.vintage_catalog = TIGERVintageCatalog(year=year, catalog=catalog)\n        self.entity_metadata = self.vintage_catalog.get_entity_metadata(entity_name=self.entity_name)\n\n    @property\n    def entity_url(self):\n        return self.entity_metadata[\"metadata_url\"].values[0]\n\ntiger_tract22_obj = TIGERGeographicEntityVintage(entity_name=\"TRACT\", year=\"2022\", catalog=tiger_catalog)\ndisplay(tiger_tract22_obj.entity_metadata)\nentity_df = scrape_census_ftp_metadata_page(metadata_url=tiger_tract22_obj.entity_url)\nprint(entity_df.shape)\ndisplay(entity_df.head(2))\n\n\n\n\n\n\n\n\n\nname\nlast_modified\nsize\ndescription\nis_dir\nclean_name\nis_file\nmetadata_url\n\n\n\n\n41\nTRACT/\n2022-09-30 22:39:00\n-\n\nTrue\nTRACT\nFalse\nhttps://www2.census.gov/geo/tiger/TIGER2022/TRACT\n\n\n\n\n\n\n\n(56, 8)\n\n\n\n\n\n\n\n\n\nname\nlast_modified\nsize\ndescription\nis_dir\nclean_name\nis_file\nmetadata_url\n\n\n\n\n1\ntl_2022_01_tract.zip\n2022-10-31 19:42:00\n11M\n\nFalse\ntl_2022_01_tract.zip\nTrue\nhttps://www2.census.gov/geo/tiger/TIGER2022/TR...\n\n\n2\ntl_2022_02_tract.zip\n2022-10-31 19:42:00\n3.0M\n\nFalse\ntl_2022_02_tract.zip\nTrue\nhttps://www2.census.gov/geo/tiger/TIGER2022/TR...\n\n\n\n\n\n\n\nLooking at the names, I see that the Census groups tracts by state FIPS code (Federal Information Processing Series code). I know the FIPS code for Illinois is “17” (you can review the FIPS codes for other geographic entities here). I also know I don’t always want to pull data for all states, so I need to add a method that allows the user to filter the entity files. Also, I should build in the step of getting the entity files metadata.\n\n\nCode\nclass TIGERGeographicEntityVintage:\n    def __init__(self, entity_name: str, year: str, catalog: Optional[TIGERCatalog] = None):\n        self.entity_name = entity_name\n        self.year = str(year)\n        self.vintage_catalog = TIGERVintageCatalog(year=year, catalog=catalog)\n        self.entity_metadata = self.vintage_catalog.get_entity_metadata(entity_name=self.entity_name)\n\n    @property\n    def entity_url(self):\n        return self.entity_metadata[\"metadata_url\"].values[0]\n\n    @property\n    def entity_files_metadata(self):\n        return scrape_census_ftp_metadata_page(metadata_url=self.entity_url)\n\n    def get_entity_file_metadata(self, filter_str: str) -&gt; pd.DataFrame:\n        return self.entity_files_metadata.loc[self.entity_files_metadata[\"name\"].str.contains(filter_str)].copy()\n\ntiger_tract22_obj = TIGERGeographicEntityVintage(entity_name=\"TRACT\", year=\"2022\", catalog=tiger_catalog)\nil_tracts22_metadata = tiger_tract22_obj.get_entity_file_metadata(filter_str=\"_17_\")\ndisplay(il_tracts22_metadata)\nprint(f\"IL Tracts archive download Url: {il_tracts22_metadata['metadata_url'].values[0]}\")\n\n\n\n\n\n\n\n\n\nname\nlast_modified\nsize\ndescription\nis_dir\nclean_name\nis_file\nmetadata_url\n\n\n\n\n14\ntl_2022_17_tract.zip\n2022-10-31 19:43:00\n9.5M\n\nFalse\ntl_2022_17_tract.zip\nTrue\nhttps://www2.census.gov/geo/tiger/TIGER2022/TR...\n\n\n\n\n\n\n\nIL Tracts archive download Url: https://www2.census.gov/geo/tiger/TIGER2022/TRACT/tl_2022_17_tract.zip\n\n\nGeopandas provides some extremely convenient functionality for loading data. I can provide the URL to a zipped archive (of geospatial files) to geopandas’ read_file() function and it handles the network request and unzipping for me.\n\n\nCode\nil_tracts_gdf = gpd.read_file(il_tracts22_metadata[\"metadata_url\"].values[0])\n\n\n\n\nCode\nprint(il_tracts_gdf.shape)\nil_tracts_gdf.head(2)\n\n\n(3265, 13)\n\n\n\n\n\n\n\n\n\nSTATEFP\nCOUNTYFP\nTRACTCE\nGEOID\nNAME\nNAMELSAD\nMTFCC\nFUNCSTAT\nALAND\nAWATER\nINTPTLAT\nINTPTLON\ngeometry\n\n\n\n\n0\n17\n019\n010701\n17019010701\n107.01\nCensus Tract 107.01\nG5020\nS\n5266000\n30553\n+40.1150269\n-088.0329549\nPOLYGON ((-88.05240 40.11923, -88.05238 40.119...\n\n\n1\n17\n019\n005902\n17019005902\n59.02\nCensus Tract 59.02\nG5020\nS\n962402\n4892\n+40.1087344\n-088.2247204\nPOLYGON ((-88.22891 40.11271, -88.22882 40.112...\n\n\n\n\n\n\n\nAnd now I have plottable, spatially-joinable geospatial data ready to ingest into a data warehouse table or to plot out.\n\n\nCode\nfig_width = 14\n\nfig, ax = plt.subplots(figsize=(fig_width, fig_width))\nax = il_tracts_gdf.plot(facecolor=\"none\", edgecolor=\"black\", linewidth=0.015 * fig_width, ax=ax)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNo matching items"
  }
]